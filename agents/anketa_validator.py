#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AnketaValidator - Validates anketa data quality before generation

ITERATION 37: Two-Stage Quality Control
GATE 1: Validate INPUT (anketa JSON)

Purpose: Prevent garbage data from entering generation pipeline
- Checks required fields exist
- Validates minimum data lengths
- Uses LLM to assess coherence and completeness
- Returns validation score and actionable recommendations

Author: Claude Code (Iteration 37)
Date: 2025-10-25
Version: 1.0
"""

import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging
import json

# Add paths
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "shared"))

from shared.llm.unified_llm_client import UnifiedLLMClient

logger = logging.getLogger(__name__)


class AnketaValidator:
    """
    Validates anketa JSON data quality

    GATE 1 in Two-Stage Quality Pipeline:
    - Input: Raw anketa JSON (dict)
    - Output: Validation score (0-10), issues, recommendations

    Example:
        validator = AnketaValidator(llm_provider='gigachat')
        result = await validator.validate(anketa_data)

        if result['score'] < 5.0:
            print("Anketa quality too low:", result['issues'])
        else:
            print("Anketa OK, can proceed to generation")
    """

    # Required fields for minimal anketa
    REQUIRED_FIELDS = [
        'project_name',
        'problem',
        'solution',
        'goals',
        'budget'
    ]

    # Minimum character lengths for text fields
    MIN_LENGTHS = {
        'project_name': 10,
        'problem': 200,
        'solution': 150,
        'goals': 50  # Total for all goals if list
    }

    # Score thresholds
    SCORE_THRESHOLD_GOOD = 7.0      # Can proceed with confidence
    SCORE_THRESHOLD_ACCEPTABLE = 5.0  # Can proceed but with warnings
    SCORE_THRESHOLD_POOR = 3.0       # Should not proceed

    def __init__(
        self,
        llm_provider: str = 'gigachat',
        db=None
    ):
        """
        Args:
            llm_provider: LLM provider to use (gigachat, claude_code, etc)
            db: Database instance (optional)
        """
        self.llm_provider = llm_provider
        self.db = db
        self.llm_client = UnifiedLLMClient(provider=llm_provider)

        logger.info(f"[AnketaValidator] Initialized with provider: {llm_provider}")

    def _check_required_fields(self, anketa_data: Dict) -> List[str]:
        """
        Check if all required fields are present and non-empty

        Args:
            anketa_data: Anketa dict

        Returns:
            List of missing/empty field names
        """
        missing = []

        for field in self.REQUIRED_FIELDS:
            value = anketa_data.get(field)

            # Check if field exists and is not empty
            if not value:
                missing.append(field)
            elif isinstance(value, str) and len(value.strip()) == 0:
                missing.append(field)
            elif isinstance(value, list) and len(value) == 0:
                missing.append(field)

        return missing

    def _check_minimum_lengths(self, anketa_data: Dict) -> List[Dict[str, Any]]:
        """
        Check if text fields meet minimum length requirements

        Args:
            anketa_data: Anketa dict

        Returns:
            List of length violations: [{'field': 'problem', 'current': 50, 'required': 200}]
        """
        violations = []

        for field, min_length in self.MIN_LENGTHS.items():
            value = anketa_data.get(field)

            if not value:
                continue  # Already caught by required_fields check

            # Calculate actual length
            if isinstance(value, str):
                actual_length = len(value.strip())
            elif isinstance(value, list):
                # For lists (like goals), sum all text
                actual_length = sum(len(str(item)) for item in value)
            else:
                actual_length = len(str(value))

            # Check if below minimum
            if actual_length < min_length:
                violations.append({
                    'field': field,
                    'current': actual_length,
                    'required': min_length,
                    'deficit': min_length - actual_length
                })

        return violations

    def _build_llm_validation_prompt(self, anketa_data: Dict) -> str:
        """
        Build prompt for LLM to assess anketa coherence and completeness

        Args:
            anketa_data: Anketa dict

        Returns:
            str: Prompt for LLM
        """
        # Format anketa for LLM (pretty JSON)
        anketa_json = json.dumps(anketa_data, ensure_ascii=False, indent=2)

        prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ –∞–Ω–∫–µ—Ç –¥–ª—è –≥—Ä–∞–Ω—Ç–æ–≤—ã—Ö –∑–∞—è–≤–æ–∫.

–ó–ê–î–ê–ß–ê: –û—Ü–µ–Ω–∏ –î–û–°–¢–ê–¢–û–ß–ù–û–°–¢–¨ –∏ –ö–ê–ß–ï–°–¢–í–û –¥–∞–Ω–Ω—ã—Ö –≤ –∞–Ω–∫–µ—Ç–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏.

–ê–ù–ö–ï–¢–ê (JSON):
```json
{anketa_json}
```

–ö–†–ò–¢–ï–†–ò–ò –û–¶–ï–ù–ö–ò:

1. **–ü–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã** (0-3 –±–∞–ª–ª–∞):
   - –ï—Å—Ç—å –ª–∏ —á—ë—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã?
   - –£–∫–∞–∑–∞–Ω—ã –ª–∏ –ø—Ä–∏—á–∏–Ω—ã –∏ –º–∞—Å—à—Ç–∞–± –ø—Ä–æ–±–ª–µ–º—ã?
   - –ü–æ–Ω—è—Ç–Ω–æ –ª–∏, –∫–æ–≥–æ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º–∞?

2. **–ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏—è** (0-3 –±–∞–ª–ª–∞):
   - –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ?
   - –°–≤—è–∑–∞–Ω–æ –ª–∏ —Ä–µ—à–µ–Ω–∏–µ —Å –ø—Ä–æ–±–ª–µ–º–æ–π?
   - –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ –ª–∏ —Ä–µ—à–µ–Ω–∏–µ?

3. **–¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏** (0-2 –±–∞–ª–ª–∞):
   - –£–∫–∞–∑–∞–Ω—ã –ª–∏ —Ü–µ–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞?
   - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã –ª–∏ —Ü–µ–ª–∏?
   - –ï—Å—Ç—å –ª–∏ –∑–∞–¥–∞—á–∏/–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è?

4. **–ë—é–¥–∂–µ—Ç –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ** (0-2 –±–∞–ª–ª–∞):
   - –£–∫–∞–∑–∞–Ω –ª–∏ –±—é–¥–∂–µ—Ç?
   - –ï—Å—Ç—å –ª–∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –±—é–¥–∂–µ—Ç–∞?

–û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê (0-10):
- 8-10: –û—Ç–ª–∏—á–Ω–∞—è –∞–Ω–∫–µ—Ç–∞, –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å, –º–æ–∂–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞—è–≤–∫—É –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- 5-7: –•–æ—Ä–æ—à–∞—è –∞–Ω–∫–µ—Ç–∞, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –Ω–æ –µ—Å—Ç—å –ø—Ä–æ–±–µ–ª—ã
- 3-4: –°–ª–∞–±–∞—è –∞–Ω–∫–µ—Ç–∞, –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- 0-2: –û—á–µ–Ω—å –ø–ª–æ—Ö–∞—è –∞–Ω–∫–µ—Ç–∞, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

–í–ê–ñ–ù–û:
- –û—Ü–µ–Ω–∏–≤–∞–π –î–ê–ù–ù–´–ï, –∞ –Ω–µ —Ñ–æ—Ä–º–∞—Ç JSON
- –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ –∏–ª–∏ –æ–Ω–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ - —Å–Ω–∏–∂–∞–π –æ—Ü–µ–Ω–∫—É
- –ë—É–¥—å —Å—Ç—Ä–æ–≥–∏–º: –ª—É—á—à–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –ø–ª–æ—Ö—É—é –∞–Ω–∫–µ—Ç—É, —á–µ–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º—É—Å–æ—Ä

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
{{
  "score": X.X,
  "problem_score": X,
  "solution_score": X,
  "goals_score": X,
  "budget_score": X,
  "issues": [
    "–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ 1",
    "–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ 2"
  ],
  "recommendations": [
    "—á—Ç–æ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å 1",
    "—á—Ç–æ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å 2"
  ],
  "can_proceed": true/false,
  "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)"
}}

–ù–ê–ß–ò–ù–ê–ô –ê–ù–ê–õ–ò–ó:"""

        return prompt

    async def _llm_coherence_check(self, anketa_data: Dict) -> Dict[str, Any]:
        """
        Use LLM to assess anketa coherence and quality

        Args:
            anketa_data: Anketa dict

        Returns:
            dict: {
                'score': float,
                'issues': [...],
                'recommendations': [...],
                'can_proceed': bool,
                'reasoning': str
            }
        """
        try:
            # Build prompt
            prompt = self._build_llm_validation_prompt(anketa_data)

            logger.info(f"[AnketaValidator] Running LLM coherence check...")

            # Call LLM
            async with self.llm_client as client:
                response = await client.generate_text(
                    prompt=prompt,
                    max_tokens=1500
                )

            logger.info(f"[AnketaValidator] LLM response: {len(response)} chars")

            # Parse JSON response
            # LLM might wrap in ```json ... ``` or add extra text - clean it
            response = response.strip()

            # Remove code block markers
            if response.startswith('```json'):
                response = response[7:]
            if response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]

            response = response.strip()

            # Try to extract JSON object from response
            # GigaChat may add text after JSON - find first { and last }
            try:
                # Find JSON boundaries
                start = response.find('{')
                end = response.rfind('}')

                if start != -1 and end != -1 and end > start:
                    json_str = response[start:end+1]
                    result = json.loads(json_str)
                else:
                    # Fallback: try to parse as-is
                    result = json.loads(response)
            except json.JSONDecodeError as e:
                # Last resort: try line-by-line to find valid JSON
                logger.warning(f"[AnketaValidator] JSON parse failed, trying to extract: {e}")
                lines = response.split('\n')
                json_lines = []
                in_json = False

                for line in lines:
                    if '{' in line:
                        in_json = True
                    if in_json:
                        json_lines.append(line)
                    if '}' in line and in_json:
                        break

                json_str = '\n'.join(json_lines)
                result = json.loads(json_str)

            # Validate structure
            if 'score' not in result:
                raise ValueError("LLM response missing 'score' field")

            logger.info(f"[AnketaValidator] LLM score: {result['score']}/10")

            return {
                'score': float(result.get('score', 0)),
                'problem_score': result.get('problem_score', 0),
                'solution_score': result.get('solution_score', 0),
                'goals_score': result.get('goals_score', 0),
                'budget_score': result.get('budget_score', 0),
                'issues': result.get('issues', []),
                'recommendations': result.get('recommendations', []),
                'can_proceed': result.get('can_proceed', False),
                'reasoning': result.get('reasoning', '')
            }

        except Exception as e:
            logger.error(f"[AnketaValidator] LLM coherence check failed: {e}")
            # Fallback: allow to proceed but with warning
            return {
                'score': 5.0,  # Neutral score
                'issues': [f"LLM validation failed: {str(e)}"],
                'recommendations': ["LLM validation was unavailable"],
                'can_proceed': True,  # Don't block on LLM error
                'reasoning': 'LLM validation error - proceeding with caution'
            }

    def _calculate_final_score(
        self,
        missing_fields: List[str],
        length_violations: List[Dict],
        llm_result: Dict
    ) -> float:
        """
        Calculate final validation score

        Args:
            missing_fields: List of missing required fields
            length_violations: List of length violations
            llm_result: LLM coherence check result

        Returns:
            float: Final score (0-10)
        """
        # Start with LLM score
        score = llm_result['score']

        # Penalize missing required fields heavily
        missing_penalty = len(missing_fields) * 2.0
        score -= missing_penalty

        # Penalize length violations moderately
        length_penalty = len(length_violations) * 0.5
        score -= length_penalty

        # Floor at 0
        score = max(0.0, score)

        logger.info(
            f"[AnketaValidator] Final score: {score:.1f}/10 "
            f"(LLM: {llm_result['score']:.1f}, "
            f"missing_penalty: -{missing_penalty}, "
            f"length_penalty: -{length_penalty})"
        )

        return score

    async def validate(self, anketa_data: Dict) -> Dict[str, Any]:
        """
        Validate anketa data quality

        This is the main method - validates anketa before generation

        Args:
            anketa_data: Anketa JSON dict

        Returns:
            {
                'valid': bool,              # True if score >= 5.0
                'score': float,             # 0-10
                'issues': [...],            # List of problems found
                'recommendations': [...],   # What to improve
                'can_proceed': bool,        # True if can proceed to generation
                'details': {
                    'missing_fields': [...],
                    'length_violations': [...],
                    'llm_assessment': {...}
                }
            }
        """
        logger.info("")
        logger.info("=" * 60)
        logger.info("üîç ANKETA VALIDATOR - STARTING")
        logger.info("=" * 60)

        # 1. Check required fields
        logger.info("[1/3] Checking required fields...")
        missing_fields = self._check_required_fields(anketa_data)

        if missing_fields:
            logger.warning(f"‚ö†Ô∏è Missing fields: {missing_fields}")
        else:
            logger.info("‚úÖ All required fields present")

        # 2. Check minimum lengths
        logger.info("[2/3] Checking minimum lengths...")
        length_violations = self._check_minimum_lengths(anketa_data)

        if length_violations:
            logger.warning(f"‚ö†Ô∏è Length violations: {len(length_violations)}")
            for v in length_violations:
                logger.warning(f"   - {v['field']}: {v['current']}/{v['required']} chars")
        else:
            logger.info("‚úÖ All fields meet minimum length")

        # 3. LLM coherence check
        logger.info("[3/3] Running LLM coherence check...")
        llm_result = await self._llm_coherence_check(anketa_data)

        logger.info(f"‚úÖ LLM assessment complete: {llm_result['score']:.1f}/10")

        # 4. Calculate final score
        final_score = self._calculate_final_score(
            missing_fields,
            length_violations,
            llm_result
        )

        # 5. Compile all issues
        all_issues = []

        # Add missing fields to issues
        if missing_fields:
            all_issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {', '.join(missing_fields)}")

        # Add length violations to issues
        for v in length_violations:
            all_issues.append(
                f"–ü–æ–ª–µ '{v['field']}' —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ: {v['current']} —Å–∏–º–≤–æ–ª–æ–≤ "
                f"(–º–∏–Ω–∏–º—É–º {v['required']})"
            )

        # Add LLM issues
        all_issues.extend(llm_result.get('issues', []))

        # 6. Compile recommendations
        all_recommendations = []

        if missing_fields:
            all_recommendations.append(f"–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–æ–ª—è: {', '.join(missing_fields)}")

        for v in length_violations:
            all_recommendations.append(
                f"–†–∞—Å—à–∏—Ä—å—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ –ø–æ–ª–µ '{v['field']}' "
                f"(–Ω—É–∂–Ω–æ –µ—â—ë {v['deficit']} —Å–∏–º–≤–æ–ª–æ–≤)"
            )

        all_recommendations.extend(llm_result.get('recommendations', []))

        # 7. Determine if can proceed
        can_proceed = (
            final_score >= self.SCORE_THRESHOLD_ACCEPTABLE and
            len(missing_fields) == 0  # All required fields must be present
        )

        # 8. Final result
        result = {
            'valid': final_score >= self.SCORE_THRESHOLD_ACCEPTABLE,
            'score': final_score,
            'issues': all_issues,
            'recommendations': all_recommendations,
            'can_proceed': can_proceed,
            'details': {
                'missing_fields': missing_fields,
                'length_violations': length_violations,
                'llm_assessment': llm_result
            }
        }

        # 9. Log summary
        logger.info("")
        logger.info("=" * 60)
        logger.info("‚úÖ ANKETA VALIDATOR - COMPLETE")
        logger.info("=" * 60)
        logger.info(f"üìä Final Score: {final_score:.1f}/10")
        logger.info(f"‚úÖ Valid: {result['valid']}")
        logger.info(f"üö¶ Can Proceed: {can_proceed}")
        logger.info(f"‚ö†Ô∏è Issues: {len(all_issues)}")
        logger.info(f"üí° Recommendations: {len(all_recommendations)}")
        logger.info("")

        return result


if __name__ == "__main__":
    """
    Quick test of AnketaValidator
    """
    import asyncio

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Test data: good anketa
    good_anketa = {
        'project_name': '–ú–æ–ª–æ–¥–µ–∂–Ω—ã–π –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä "–¶–∏—Ñ—Ä–æ–≤–æ–µ –±—É–¥—É—â–µ–µ"',
        'problem': """–í –ö–µ–º–µ—Ä–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –æ—Å—Ç—Ä—ã–π –¥–µ—Ñ–∏—Ü–∏—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö
        –ø—Ä–æ–≥—Ä–∞–º–º –ø–æ —Ü–∏—Ñ—Ä–æ–≤—ã–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º –¥–ª—è –º–æ–ª–æ–¥–µ–∂–∏ –∏–∑ –º–∞–ª—ã—Ö –≥–æ—Ä–æ–¥–æ–≤. –°–æ–≥–ª–∞—Å–Ω–æ –¥–∞–Ω–Ω—ã–º
        —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–∞ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, —Ç–æ–ª—å–∫–æ 15% –º–æ–ª–æ–¥—ã—Ö –ª—é–¥–µ–π –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 14-25 –ª–µ—Ç
        –∏–º–µ—é—Ç –¥–æ—Å—Ç—É–ø –∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –≤ —Å—Ñ–µ—Ä–µ IT. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –æ—Ç—Ç–æ–∫—É —Ç–∞–ª–∞–Ω—Ç–ª–∏–≤–æ–π
        –º–æ–ª–æ–¥–µ–∂–∏ –≤ –∫—Ä—É–ø–Ω—ã–µ –≥–æ—Ä–æ–¥–∞ –∏ —Å–Ω–∏–∂–µ–Ω–∏—é –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ä–µ–≥–∏–æ–Ω–∞.""",
        'solution': """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–ª–æ–¥–µ–∂–Ω–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ –∫—É—Ä—Å–∞–º–∏ –ø–æ
        –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é, –≤–µ–±-–¥–∏–∑–∞–π–Ω—É, –∏ —Ü–∏—Ñ—Ä–æ–≤–æ–º—É –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É. –¶–µ–Ω—Ç—Ä –±—É–¥–µ—Ç –æ—Å–Ω–∞—â–µ–Ω —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º
        –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –±–∞–∑–µ –º–µ—Å—Ç–Ω–æ–≥–æ –î–ö.""",
        'goals': [
            '–û–±—É—á–∏—Ç—å 200 –º–æ–ª–æ–¥—ã—Ö –ª—é–¥–µ–π —Ü–∏—Ñ—Ä–æ–≤—ã–º –Ω–∞–≤—ã–∫–∞–º',
            '–°–æ–∑–¥–∞—Ç—å 3 –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã',
            '–û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å —Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∏ –¥–ª—è 50 –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤'
        ],
        'budget': '850000'
    }

    # Test data: poor anketa
    poor_anketa = {
        'project_name': '–ü—Ä–æ–µ–∫—Ç',
        'problem': '–ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–∞',
        'solution': '',
        'goals': [],
        'budget': '0'
    }

    async def test():
        print("\n" + "="*60)
        print("TEST 1: Good Anketa")
        print("="*60)

        validator = AnketaValidator(llm_provider='gigachat')
        result = await validator.validate(good_anketa)

        print(f"\nRESULT:")
        print(f"  Score: {result['score']:.1f}/10")
        print(f"  Valid: {result['valid']}")
        print(f"  Can Proceed: {result['can_proceed']}")
        print(f"  Issues: {len(result['issues'])}")

        print("\n" + "="*60)
        print("TEST 2: Poor Anketa")
        print("="*60)

        result2 = await validator.validate(poor_anketa)

        print(f"\nRESULT:")
        print(f"  Score: {result2['score']:.1f}/10")
        print(f"  Valid: {result2['valid']}")
        print(f"  Can Proceed: {result2['can_proceed']}")
        print(f"  Issues: {len(result2['issues'])}")
        if result2['issues']:
            print(f"\nISSUES:")
            for issue in result2['issues']:
                print(f"  - {issue}")

    asyncio.run(test())
