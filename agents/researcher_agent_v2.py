#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Researcher Agent V2 - Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· 27 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²

ĞĞ Ğ¥Ğ˜Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ (Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ 2025-10-11, v2.3):
- Database-Driven: WebSearch Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· ai_agent_settings.config.websearch_provider
- WebSearchRouter: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ° Ñ fallback (Perplexity â†’ Claude Code)
- ĞĞ• Ñ…Ğ°Ñ€Ğ´ĞºĞ¾Ğ´Ğ¸Ğ¼: ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· UI, Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚:
- DatabasePromptManager Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ 27 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ‘Ğ” (3 Ğ±Ğ»Ğ¾ĞºĞ°: 10+10+7)
- WebSearchRouter Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° WebSearch Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°
  - Perplexity API (primary): ~$0.01/Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸Ğ· Ğ Ğ¤, 100% success rate
  - Claude Code WebSearch (fallback): Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ
- researcher_research Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² JSONB

ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ v2.3:
- Ğ—Ğ°Ğ¼ĞµĞ½ĞµĞ½ PerplexityWebSearchClient Ğ½Ğ° WebSearchRouter
- ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ‘Ğ” Ñ‡ĞµÑ€ĞµĞ· get_agent_settings('researcher')
- ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° fallback Ñ‡ĞµÑ€ĞµĞ· websearch_fallback config
- Metadata ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€

ĞĞ²Ñ‚Ğ¾Ñ€: AI Integration Specialist
Ğ”Ğ°Ñ‚Ğ°: 2025-10-11
Ğ’ĞµÑ€ÑĞ¸Ñ: 2.3
"""

import sys
import os
from typing import Dict, Any, List, Optional
import logging
import asyncio
import time
from datetime import datetime
import json

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼
sys.path.append('/var/GrantService/shared')
sys.path.append('/var/GrantService/agents')
# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº web-admin (Ñ Ğ´ĞµÑ„Ğ¸ÑĞ¾Ğ¼)
current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, 'web-admin'))

from agents.base_agent import BaseAgent
from agents.prompt_loader import ResearcherPromptLoader  # ĞÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ»Ñ fallback
from shared.llm.websearch_router import WebSearchRouter

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ DatabasePromptManager
try:
    from utils.prompt_manager import DatabasePromptManager, get_database_prompt_manager
    PROMPT_MANAGER_AVAILABLE = True
except ImportError:
    print("[WARN] DatabasePromptManager Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ResearcherPromptLoader")
    PROMPT_MANAGER_AVAILABLE = False

logger = logging.getLogger(__name__)


class ResearcherAgentV2(BaseAgent):
    """
    Researcher Agent V2: 27 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°ÑĞ²Ğ¾Ğº

    Workflow:
    1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ°Ğ½ĞºĞµÑ‚Ñƒ Ğ¸Ğ· Ğ‘Ğ”
    2. Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ placeholders Ñ‡ĞµÑ€ĞµĞ· PromptLoader
    3. Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 27 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ±Ğ»Ğ¾Ğº 1: 10, Ğ±Ğ»Ğ¾Ğº 2: 10, Ğ±Ğ»Ğ¾Ğº 3: 7)
    4. Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‡ĞµÑ€ĞµĞ· PerplexityWebSearchClient (Perplexity API)
    5. ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² JSONB ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ
    6. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² researcher_research.research_results

    NOTE: Switched from Claude Code WebSearch to Perplexity API due to geographical restrictions
    """

    def __init__(self, db, llm_provider: str = "claude_code", websearch_provider: str = None, websearch_fallback: str = None):
        """
        Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°

        Args:
            db: Database instance
            llm_provider: ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ LLM (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ claude_code)
            websearch_provider: WebSearch Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾, ĞµÑĞ»Ğ¸ None - Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ‘Ğ”)
            websearch_fallback: WebSearch fallback (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾, ĞµÑĞ»Ğ¸ None - Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ‘Ğ”)
        """
        super().__init__("researcher_v2", db, llm_provider)

        # Ğ•ÑĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¸Ñ…
        if websearch_provider:
            self.websearch_provider = websearch_provider
            self.websearch_fallback = websearch_fallback or 'perplexity'
            logger.info(f"[ResearcherAgentV2] WebSearch provider (from params): {self.websearch_provider}")
            logger.info(f"[ResearcherAgentV2] WebSearch fallback (from params): {self.websearch_fallback}")
        else:
            # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ WebSearch Ğ¸Ğ· Ğ‘Ğ” (ĞĞ• Ğ·Ğ°Ñ…Ğ°Ñ€Ğ´ĞºĞ¾Ğ¶ĞµĞ½Ñ‹!)
            try:
                # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ¸Ğ· web-admin (Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ Ğ´ĞµÑ„Ğ¸ÑĞ¾Ğ¼)
                import importlib.util
                utils_path = os.path.join(current_dir, 'web-admin', 'utils', 'agent_settings.py')
                spec = importlib.util.spec_from_file_location("agent_settings", utils_path)
                agent_settings_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(agent_settings_module)
                get_agent_settings = agent_settings_module.get_agent_settings

                settings = get_agent_settings('researcher')
                config = settings.get('config', {})

                # WebSearch Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ Ğ¸Ğ· Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº (ĞĞ• Ñ…Ğ°Ñ€Ğ´ĞºĞ¾Ğ´!)
                self.websearch_provider = config.get('websearch_provider', 'perplexity')
                self.websearch_fallback = config.get('websearch_fallback', 'claude_code')

                logger.info(f"[ResearcherAgentV2] WebSearch provider from DB: {self.websearch_provider}")
                logger.info(f"[ResearcherAgentV2] WebSearch fallback from DB: {self.websearch_fallback}")

            except Exception as e:
                logger.warning(f"[ResearcherAgentV2] Failed to load WebSearch settings from DB: {e}")
                logger.info("[ResearcherAgentV2] Using defaults: websearch_provider=claude_code, NO fallback (Claude Code ONLY policy)")
                self.websearch_provider = 'claude_code'
                self.websearch_fallback = None  # NO fallback - Claude Code ONLY

        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ DatabasePromptManager
        self.prompt_manager: Optional[DatabasePromptManager] = None
        if PROMPT_MANAGER_AVAILABLE:
            try:
                self.prompt_manager = get_database_prompt_manager()
                logger.info("[OK] Researcher V2: DatabasePromptManager connected (27 queries from DB)")
            except Exception as e:
                logger.warning(f"[WARN] Could not initialize PromptManager: {e}")

        logger.info(f"[OK] ResearcherAgentV2 initialized with WebSearchRouter (provider={self.websearch_provider})")

    def _get_goal(self) -> str:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ goal Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ‘Ğ” Ñ fallback"""
        if self.prompt_manager:
            try:
                goal = self.prompt_manager.get_prompt('researcher_v2', 'goal')
                if goal:
                    return goal
            except Exception as e:
                logger.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ goal Ğ¸Ğ· Ğ‘Ğ”: {e}")

        # Fallback Ğ½Ğ° hardcoded
        return "ĞŸÑ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· 27 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… WebSearch Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°ÑĞ²ĞºĞ¸"

    def _get_backstory(self) -> str:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ backstory Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ‘Ğ” Ñ fallback"""
        if self.prompt_manager:
            try:
                backstory = self.prompt_manager.get_prompt('researcher_v2', 'backstory')
                if backstory:
                    return backstory
            except Exception as e:
                logger.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ backstory Ğ¸Ğ· Ğ‘Ğ”: {e}")

        # Fallback Ğ½Ğ° hardcoded
        return """Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ñ 15-Ğ»ĞµÑ‚Ğ½Ğ¸Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ² Ğ³Ñ€Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ½ÑĞ°Ğ»Ñ‚Ğ¸Ğ½Ğ³Ğµ.
        Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ¾ÑĞ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… ĞºĞµĞ¹ÑĞ¾Ğ².
        Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑˆÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: Ğ Ğ¾ÑÑÑ‚Ğ°Ñ‚, Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ°, Ğ½Ğ°Ñ†Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹."""

    async def research_with_expert_prompts(self, anketa_id: str) -> Dict[str, Any]:
        """
        Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ 27 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°ÑĞ²ĞºĞ¸

        Args:
            anketa_id: ID Ğ°Ğ½ĞºĞµÑ‚Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ

        Returns:
            {
                'research_id': 'RES-...',
                'status': 'completed',
                'research_results': {
                    'block1_problem': {...},
                    'block2_geography': {...},
                    'block3_goals': {...},
                    'metadata': {...}
                }
            }
        """
        start_time = time.time()

        try:
            logger.info(f"ğŸ” Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ anketa_id={anketa_id}")

            # 1. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ½ĞºĞµÑ‚Ñƒ Ğ¸Ğ· Ğ‘Ğ”
            anketa = await self._get_anketa(anketa_id)
            if not anketa:
                raise ValueError(f"Anketa {anketa_id} not found")

            logger.info(f"âœ… ĞĞ½ĞºĞµÑ‚Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°: user_id={anketa.get('user_id', anketa.get('telegram_id'))}")

            # 2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ² researcher_research (status='pending')
            research_id = await self._create_research_record(anketa_id, anketa)
            logger.info(f"ğŸ“ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {research_id}")

            # 3. ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ½Ğ° 'processing'
            await self._update_research_status(research_id, 'processing')

            # 4. Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ placeholders
            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ResearcherPromptLoader Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ placeholders
            loader = ResearcherPromptLoader()
            placeholders = loader.extract_placeholders(anketa)

            logger.info(f"ğŸ“‹ Placeholders Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ñ‹:")
            logger.info(f"   - ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ: {placeholders['ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ'][:50]}...")
            logger.info(f"   - Ğ Ğ•Ğ“Ğ˜ĞĞ: {placeholders['Ğ Ğ•Ğ“Ğ˜ĞĞ']}")
            logger.info(f"   - Ğ¡Ğ¤Ğ•Ğ Ğ: {placeholders['Ğ¡Ğ¤Ğ•Ğ Ğ']}")

            # 5. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ 27 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ‘Ğ” (DatabasePromptManager) Ğ¸Ğ»Ğ¸ fallback Ğ½Ğ° PromptLoader
            all_queries = await self._load_queries_from_db_or_fallback(placeholders)

            logger.info(f"âœ… Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹:")
            logger.info(f"   - Ğ‘Ğ»Ğ¾Ğº 1: {len(all_queries['block1'])} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")
            logger.info(f"   - Ğ‘Ğ»Ğ¾Ğº 2: {len(all_queries['block2'])} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")
            logger.info(f"   - Ğ‘Ğ»Ğ¾Ğº 3: {len(all_queries['block3'])} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")

            # 6. Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‡ĞµÑ€ĞµĞ· WebSearchRouter (Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸Ğ· Ğ‘Ğ”!)
            async with WebSearchRouter(self.db) as websearch_router:

                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒĞµ API
                healthy = await websearch_router.check_health()
                if not healthy:
                    logger.warning(f"[WARN] WebSearch provider {self.websearch_provider} not responding, attempting to continue...")

                # Ğ‘Ğ›ĞĞš 1: ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ (10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)
                logger.info("ğŸ” Ğ‘Ğ›ĞĞš 1: ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ (10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)")
                block1_results = await self._execute_block_queries(
                    block_name="block1_problem",
                    queries=all_queries['block1'],
                    websearch_client=websearch_router,  # Router Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°!
                    allowed_domains=[
                        'rosstat.gov.ru',
                        'fedstat.ru',
                        'government.ru',
                        'nationalprojects.ru',
                        f"{placeholders.get('ĞŸĞ ĞĞ¤Ğ˜Ğ›Ğ¬ĞĞĞ•_ĞœĞ˜ĞĞ˜Ğ¡Ğ¢Ğ•Ğ Ğ¡Ğ¢Ğ’Ğ', 'minsport')}.gov.ru",
                        'edu.gov.ru',
                        'minzdrav.gov.ru'
                    ],
                    placeholders=placeholders
                )

                logger.info(f"âœ… Ğ‘Ğ»Ğ¾Ğº 1 Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½: {block1_results['total_sources']} Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²")

                # ğŸ’¾ Ğ¡ĞĞ¥Ğ ĞĞĞ˜Ğ¢Ğ¬ Ğ”ĞĞĞĞ«Ğ• Ğ‘Ğ›ĞĞšĞ 1 Ğ¡Ğ ĞĞ—Ğ£!
                await self._save_block_results(research_id, 'block1_problem', block1_results)

                # Ğ‘Ğ›ĞĞš 2: Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ (10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)
                logger.info("ğŸŒ Ğ‘Ğ›ĞĞš 2: Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ (10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)")
                block2_results = await self._execute_block_queries(
                    block_name="block2_geography",
                    queries=all_queries['block2'],
                    websearch_client=websearch_router,  # Router Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°!
                    allowed_domains=[
                        'rosstat.gov.ru',
                        'fedstat.ru',
                        'government.gov.ru',
                        f"{placeholders['Ğ Ğ•Ğ“Ğ˜ĞĞ'].lower().replace(' ', '')}.gov.ru",  # Ğ ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ğ»
                        'minjust.gov.ru',
                        'asi.ru'
                    ],
                    placeholders=placeholders
                )

                logger.info(f"âœ… Ğ‘Ğ»Ğ¾Ğº 2 Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½: {block2_results['total_sources']} Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²")

                # ğŸ’¾ Ğ¡ĞĞ¥Ğ ĞĞĞ˜Ğ¢Ğ¬ Ğ”ĞĞĞĞ«Ğ• Ğ‘Ğ›ĞĞšĞ 2 Ğ¡Ğ ĞĞ—Ğ£!
                await self._save_block_results(research_id, 'block2_geography', block2_results)

                # Ğ‘Ğ›ĞĞš 3: Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ (7 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)
                logger.info("ğŸ¯ Ğ‘Ğ›ĞĞš 3: Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ (7 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)")
                block3_results = await self._execute_block_queries(
                    block_name="block3_goals",
                    queries=all_queries['block3'],
                    websearch_client=websearch_router,  # Router Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°!
                    allowed_domains=[
                        'rosstat.gov.ru',
                        'government.ru',
                        'nationalprojects.ru',
                        'asi.ru'
                    ],
                    placeholders=placeholders
                )

                logger.info(f"âœ… Ğ‘Ğ»Ğ¾Ğº 3 Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½: {block3_results['total_sources']} Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²")

                # ğŸ’¾ Ğ¡ĞĞ¥Ğ ĞĞĞ˜Ğ¢Ğ¬ Ğ”ĞĞĞĞ«Ğ• Ğ‘Ğ›ĞĞšĞ 3 Ğ¡Ğ ĞĞ—Ğ£!
                await self._save_block_results(research_id, 'block3_goals', block3_results)

                # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ (ĞµÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½)
                client_stats = {}
                if hasattr(websearch_router, 'get_statistics'):
                    try:
                        client_stats = await websearch_router.get_statistics()
                    except:
                        pass

            # 7. ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
            processing_time = time.time() - start_time

            research_results = {
                'block1_problem': block1_results,
                'block2_geography': block2_results,
                'block3_goals': block3_results,
                'metadata': {
                    'total_queries': 27,
                    'sources_count': (
                        block1_results['total_sources'] +
                        block2_results['total_sources'] +
                        block3_results['total_sources']
                    ),
                    'blocks': {
                        'block1': {
                            'queries': len(all_queries['block1']),
                            'sources': block1_results['total_sources'],
                            'processing_time': block1_results['processing_time']
                        },
                        'block2': {
                            'queries': len(all_queries['block2']),
                            'sources': block2_results['total_sources'],
                            'processing_time': block2_results['processing_time']
                        },
                        'block3': {
                            'queries': len(all_queries['block3']),
                            'sources': block3_results['total_sources'],
                            'processing_time': block3_results['processing_time']
                        }
                    },
                    'total_processing_time': int(processing_time),
                    'websearch_provider': self.websearch_provider,  # Ğ§Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ‘Ğ”!
                    'websearch_fallback': self.websearch_fallback,  # Fallback provider
                    'websearch_stats': client_stats
                }
            }

            # 8. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ‘Ğ”
            await self._update_research_results(
                research_id=research_id,
                status='completed',
                research_results=research_results,
                completed_at=datetime.now()
            )

            logger.info(f"âœ… Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
            logger.info(f"   - Research ID: {research_id}")
            logger.info(f"   - Ğ’ÑĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: 27")
            logger.info(f"   - Ğ’ÑĞµĞ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²: {research_results['metadata']['sources_count']}")
            logger.info(f"   - Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {processing_time:.2f}s")

            # ğŸ“„ ĞĞ¢ĞŸĞ ĞĞ’ĞšĞ PDF Ğ’ ĞĞ”ĞœĞ˜ĞĞ¡ĞšĞ˜Ğ™ Ğ§ĞĞ¢
            try:
                await self._send_research_pdf_to_admin(
                    research_id=research_id,
                    anketa_id=anketa_id,
                    research_results=research_results,
                    queries=all_queries
                )
            except Exception as e:
                logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ PDF Ğ² Ğ°Ğ´Ğ¼Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ñ‡Ğ°Ñ‚: {e}")
                # ĞĞµ Ğ¿Ğ°Ğ´Ğ°ĞµĞ¼ ĞµÑĞ»Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ - ÑÑ‚Ğ¾ Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾

            return {
                'research_id': research_id,
                'status': 'completed',
                'research_results': research_results
            }

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {e}", exc_info=True)

            # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ½Ğ° 'error'
            if 'research_id' in locals():
                await self._update_research_status(
                    research_id,
                    'error',
                    error_message=str(e)
                )

            return {
                'research_id': locals().get('research_id'),
                'status': 'error',
                'error': str(e)
            }

    async def _get_anketa(self, anketa_id: str) -> Optional[Dict]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ½ĞºĞµÑ‚Ñƒ Ğ¸Ğ· Ğ‘Ğ”"""
        try:
            # ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ½ĞºĞµÑ‚Ñ‹
            anketa = self.db.get_session_by_anketa_id(anketa_id)

            if not anketa:
                # ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ SQL Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
                with self.db.connect() as conn:
                    cursor = conn.cursor()
                    cursor.execute("SELECT * FROM sessions WHERE anketa_id = %s LIMIT 1", (anketa_id,))
                    row = cursor.fetchone()
                    if row:
                        anketa = self.db._dict_row(cursor, row)
                    cursor.close()

            return anketa

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ½ĞºĞµÑ‚Ñ‹ {anketa_id}: {e}")
            return None

    async def _load_queries_from_db_or_fallback(self, placeholders: Dict) -> Dict[str, List[str]]:
        """
        Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ 27 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ‘Ğ” Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ fallback Ğ½Ğ° PromptLoader

        Args:
            placeholders: Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸

        Returns:
            {'block1': [...], 'block2': [...], 'block3': [...]}
        """
        # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸Ğ· Ğ‘Ğ” Ñ‡ĞµÑ€ĞµĞ· DatabasePromptManager
        if self.prompt_manager:
            try:
                logger.info("ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ‘Ğ”...")

                # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°
                block1_templates = self.prompt_manager.get_researcher_queries(1)
                block2_templates = self.prompt_manager.get_researcher_queries(2)
                block3_templates = self.prompt_manager.get_researcher_queries(3)

                logger.info(f"âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ¸Ğ· Ğ‘Ğ”: {len(block1_templates)} + {len(block2_templates)} + {len(block3_templates)} = {len(block1_templates) + len(block2_templates) + len(block3_templates)} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")

                # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ placeholders Ğº ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ°Ğ¼
                block1_queries = [template.format(**placeholders) for template in block1_templates]
                block2_queries = [template.format(**placeholders) for template in block2_templates]
                block3_queries = [template.format(**placeholders) for template in block3_templates]

                return {
                    'block1': block1_queries,
                    'block2': block2_queries,
                    'block3': block3_queries
                }

            except Exception as e:
                logger.warning(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ‘Ğ”: {e}")
                logger.info("Fallback Ğ½Ğ° ResearcherPromptLoader...")

        # Fallback Ğ½Ğ° PromptLoader (hardcoded queries)
        logger.info("ğŸ“¥ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ResearcherPromptLoader (fallback)...")
        loader = ResearcherPromptLoader()
        all_queries = loader.get_all_queries(placeholders)

        return all_queries

    async def _create_research_record(self, anketa_id: str, anketa: Dict) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ‘Ğ” Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ¾Ğ¼ĞµĞ½ĞºĞ»Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹"""
        # âœ… ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞĞ¯ ĞĞĞœĞ•ĞĞšĞ›ĞĞ¢Ğ£Ğ Ğ: anketa_id-RS-001
        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ research_id Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ‘Ğ” (Ğ½Ğ¾Ğ¼ĞµĞ½ĞºĞ»Ğ°Ñ‚ÑƒÑ€Ğ°: #AN-DATE-username-NNN-RS-NNN)
        # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹: #AN-20251008-ekaterina_maksimova-001-RS-001

        user_id = anketa.get('user_id', anketa.get('telegram_id', 0))
        session_id = anketa.get('id', anketa.get('session_id'))

        research_data = {
            # ĞĞ• Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼ research_id - db.save_research_results ÑĞ°Ğ¼ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· generate_research_id()
            'anketa_id': anketa_id,
            'user_id': user_id,
            'session_id': session_id,
            'research_type': 'expert_websearch_27_queries',
            'llm_provider': self.websearch_provider,  # Ğ§Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ‘Ğ”!
            'model': 'router',  # WebSearchRouter Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            'status': 'pending',
            'created_at': datetime.now(),
            'research_results': {}
        }

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ‘Ğ”
        saved_id = self.db.save_research_results(research_data)

        # Ğ’ĞĞ–ĞĞ: Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ID Ğ¸Ğ· Ğ‘Ğ”, Ğ° Ğ½Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ!
        return saved_id

    async def _update_research_status(
        self,
        research_id: str,
        status: str,
        error_message: Optional[str] = None
    ):
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""
        try:
            with self.db.connect() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE researcher_research
                    SET status = %s
                    WHERE research_id = %s
                    """,
                    (status, research_id)
                )
                conn.commit()
                cursor.close()

            logger.info(f"ğŸ“ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½: {research_id} â†’ {status}")

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°: {e}")

    async def _update_research_results(
        self,
        research_id: str,
        status: str,
        research_results: Dict,
        completed_at: datetime
    ):
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""
        try:
            # PostgreSQL: JSONB
            results_json = json.dumps(research_results, ensure_ascii=False)

            with self.db.connect() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE researcher_research
                    SET status = %s,
                        research_results = %s::jsonb,
                        completed_at = %s
                    WHERE research_id = %s
                    """,
                    (status, results_json, completed_at, research_id)
                )
                conn.commit()
                cursor.close()

            logger.info(f"ğŸ’¾ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {research_id}")

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: {e}")

    async def _save_block_results(
        self,
        research_id: str,
        block_name: str,
        block_results: Dict
    ):
        """
        Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ»Ğ¾ĞºĞ° Ğ¡Ğ ĞĞ—Ğ£ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ

        Ğ˜Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ - Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ±Ğ»Ğ¾Ğº Ğ² JSONB
        Ğ¢Ğ°Ğº Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾Ğ¹Ğ´ĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑÑ
        """
        try:
            # ĞŸÑ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
            with self.db.connect() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    SELECT research_results
                    FROM researcher_research
                    WHERE research_id = %s
                    """,
                    (research_id,)
                )
                row = cursor.fetchone()

                if row and row[0]:
                    current_results = row[0]
                else:
                    current_results = {}

                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº
                current_results[block_name] = block_results

                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾
                results_json = json.dumps(current_results, ensure_ascii=False)
                cursor.execute(
                    """
                    UPDATE researcher_research
                    SET research_results = %s::jsonb
                    WHERE research_id = %s
                    """,
                    (results_json, research_id)
                )
                conn.commit()
                cursor.close()

            logger.info(f"ğŸ’¾ Ğ‘Ğ»Ğ¾Ğº {block_name} ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {block_results['total_sources']} Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²")

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ¾ĞºĞ° {block_name}: {e}")

    async def _execute_block_queries(
        self,
        block_name: str,
        queries: List[str],
        websearch_client: WebSearchRouter,  # Router Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°!
        allowed_domains: List[str],
        placeholders: Dict
    ) -> Dict:
        """
        Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ°

        Args:
            block_name: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ»Ğ¾ĞºĞ° (block1_problem, block2_geography, block3_goals)
            queries: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
            websearch_client: WebSearchRouter Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°
            allowed_domains: Ğ Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹
            placeholders: Placeholders Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸

        Returns:
            {
                'summary': 'Ğ ĞµĞ·ÑĞ¼Ğµ Ğ±Ğ»Ğ¾ĞºĞ°',
                'key_facts': [...],
                'sources': [...],
                'queries_used': [...],
                'total_sources': 15,
                'processing_time': 120
            }
        """
        start_time = time.time()

        logger.info(f"   Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ {len(queries)} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ»Ğ¾ĞºĞ° {block_name}...")

        # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ batch WebSearch
        batch_results = await websearch_client.batch_websearch(
            queries=queries,
            allowed_domains=allowed_domains,
            max_results=5,
            max_concurrent=3
        )

        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
        all_results = []
        all_sources = []

        # WebSearchRouter Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğµ Ğ¾Ğ±ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ² {'result': ...}
        for result in batch_results:
            all_results.append(result)

            # Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸
            if result.get('sources'):
                all_sources.extend(result['sources'])

        # Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸
        unique_sources = list(set(all_sources))

        # ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ»Ğ¾ĞºĞ°
        block_results = await self._aggregate_block_results(
            block_name=block_name,
            query_results=all_results,
            queries_used=queries,
            placeholders=placeholders
        )

        block_results['sources'] = unique_sources
        block_results['total_sources'] = len(unique_sources)
        block_results['processing_time'] = int(time.time() - start_time)

        logger.info(f"   âœ… Ğ‘Ğ»Ğ¾Ğº Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½: {len(unique_sources)} Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ·Ğ° {block_results['processing_time']}s")

        return block_results

    async def _aggregate_block_results(
        self,
        block_name: str,
        query_results: List[Dict],
        queries_used: List[str],
        placeholders: Dict
    ) -> Dict:
        """
        ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ±Ğ»Ğ¾ĞºĞ°

        TODO: Ğ—Ğ´ĞµÑÑŒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ LLM Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        ĞŸĞ¾ĞºĞ° Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ
        """
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ñ‚ĞµĞºÑÑ‚
        all_text = ""
        for result in query_results:
            for item in result.get('results', []):
                all_text += f"{item.get('title', '')} {item.get('snippet', '')} "

        # ĞŸĞ¾Ğ´ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°)
        key_facts = []
        for result in query_results:
            for item in result.get('results', []):
                if item.get('snippet'):
                    key_facts.append({
                        'fact': item['snippet'][:200],
                        'source': item.get('source', 'unknown'),
                        'url': item.get('url', ''),
                        'title': item.get('title', '')
                    })

        return {
            'summary': all_text[:500] if all_text else 'ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…',
            'key_facts': key_facts[:10],  # Ğ¢Ğ¾Ğ¿ 10 Ñ„Ğ°ĞºÑ‚Ğ¾Ğ²
            'queries_used': queries_used,
            'raw_results': query_results  # Ğ”Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        }

    def _generate_research_report_md(
        self,
        research_id: str,
        anketa_id: str,
        research_results: Dict,
        queries: Dict[str, List[str]],
        user_info: Dict
    ) -> str:
        """
        Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ MD Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

        Args:
            research_id: ID Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
            anketa_id: ID Ğ°Ğ½ĞºĞµÑ‚Ñ‹
            research_results: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
            queries: Ğ’ÑĞµ 27 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (dict Ñ block1, block2, block3)
            user_info: Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ

        Returns:
            str: MD Ğ¾Ñ‚Ñ‡ĞµÑ‚
        """
        md_lines = []

        # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
        md_lines.append("# ĞÑ‚Ñ‡Ñ‘Ñ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°ÑĞ²ĞºĞ¸")
        md_lines.append("")

        # ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        full_name = f"{user_info.get('first_name', '')} {user_info.get('last_name', '')}".strip() or user_info.get('username', 'Unknown')
        md_lines.append("## ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
        md_lines.append("")
        md_lines.append(f"- **ID ĞĞ½ĞºĞµÑ‚Ñ‹**: {anketa_id}")
        md_lines.append(f"- **ID Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ**: {research_id}")
        md_lines.append(f"- **ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ**: {full_name} (@{user_info.get('username', 'unknown')})")
        md_lines.append(f"- **Telegram ID**: {user_info.get('telegram_id', 'unknown')}")
        md_lines.append(f"- **Ğ”Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append(f"- **Ğ’ÑĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²**: 27")
        md_lines.append(f"- **Ğ’ÑĞµĞ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²**: {research_results['metadata']['sources_count']}")
        md_lines.append(f"- **Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸**: {research_results['metadata']['total_processing_time']}s")
        md_lines.append(f"- **WebSearch Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€**: {research_results['metadata']['websearch_provider']}")
        md_lines.append("")

        # Ğ‘Ğ»Ğ¾Ğº 1: ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ (10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)
        md_lines.append("## Ğ‘Ğ»Ğ¾Ğº 1: ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ")
        md_lines.append("")
        md_lines.append(f"**Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²**: {len(queries['block1'])}")
        md_lines.append(f"**Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²**: {research_results['block1_problem']['total_sources']}")
        md_lines.append(f"**Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸**: {research_results['block1_problem']['processing_time']}s")
        md_lines.append("")

        block1_results = research_results.get('block1_problem', {})
        raw_results = block1_results.get('raw_results', [])

        for i, query in enumerate(queries['block1'], 1):
            md_lines.append(f"### ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i}")
            md_lines.append("")
            md_lines.append(f"**â“ Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ:**")
            md_lines.append("")
            md_lines.append(f"> {query}")
            md_lines.append("")

            query_result = raw_results[i-1] if i-1 < len(raw_results) else {}

            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· 'content' (Ğ½Ğµ snippet!)
            full_answer = query_result.get('content', '')

            if full_answer:
                md_lines.append(f"**ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚:**")
                md_lines.append("")
                # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼ Ğ´Ğ»Ñ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸
                paragraphs = full_answer.split('\n\n')
                for para in paragraphs:
                    if para.strip():
                        md_lines.append(para.strip())
                        md_lines.append("")

                # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· 'results' (Ñ‚Ğ¾Ğ¿ 5 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹)
                if query_result.get('results'):
                    sources = [r.get('url', '') for r in query_result['results'] if r.get('url')][:5]
                    if sources:
                        md_lines.append(f"**ğŸ”— Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ ({len(sources)}):**")
                        md_lines.append("")
                        for idx, source in enumerate(sources, 1):
                            md_lines.append(f"{idx}. {source}")
                        md_lines.append("")
            else:
                md_lines.append("**ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚:** ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
                md_lines.append("")

            # Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
            md_lines.append("---")
            md_lines.append("")

        # Ğ‘Ğ»Ğ¾Ğº 2: Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ (10 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)
        md_lines.append("## Ğ‘Ğ»Ğ¾Ğº 2: Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ")
        md_lines.append("")
        md_lines.append(f"**Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²**: {len(queries['block2'])}")
        md_lines.append(f"**Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²**: {research_results['block2_geography']['total_sources']}")
        md_lines.append(f"**Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸**: {research_results['block2_geography']['processing_time']}s")
        md_lines.append("")

        block2_results = research_results.get('block2_geography', {})
        raw_results = block2_results.get('raw_results', [])

        for i, query in enumerate(queries['block2'], 1):
            md_lines.append(f"### ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i + 10}")
            md_lines.append("")
            md_lines.append(f"**â“ Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ:**")
            md_lines.append("")
            md_lines.append(f"> {query}")
            md_lines.append("")

            query_result = raw_results[i-1] if i-1 < len(raw_results) else {}

            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· 'content' (Ğ½Ğµ snippet!)
            full_answer = query_result.get('content', '')

            if full_answer:
                md_lines.append(f"**ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚:**")
                md_lines.append("")
                # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼ Ğ´Ğ»Ñ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸
                paragraphs = full_answer.split('\n\n')
                for para in paragraphs:
                    if para.strip():
                        md_lines.append(para.strip())
                        md_lines.append("")

                # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· 'results' (Ñ‚Ğ¾Ğ¿ 5 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹)
                if query_result.get('results'):
                    sources = [r.get('url', '') for r in query_result['results'] if r.get('url')][:5]
                    if sources:
                        md_lines.append(f"**ğŸ”— Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ ({len(sources)}):**")
                        md_lines.append("")
                        for idx, source in enumerate(sources, 1):
                            md_lines.append(f"{idx}. {source}")
                        md_lines.append("")
            else:
                md_lines.append("**ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚:** ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
                md_lines.append("")

            # Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
            md_lines.append("---")
            md_lines.append("")

        # Ğ‘Ğ»Ğ¾Ğº 3: Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ (7 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)
        md_lines.append("## Ğ‘Ğ»Ğ¾Ğº 3: Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ")
        md_lines.append("")
        md_lines.append(f"**Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²**: {len(queries['block3'])}")
        md_lines.append(f"**Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²**: {research_results['block3_goals']['total_sources']}")
        md_lines.append(f"**Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸**: {research_results['block3_goals']['processing_time']}s")
        md_lines.append("")

        block3_results = research_results.get('block3_goals', {})
        raw_results = block3_results.get('raw_results', [])

        for i, query in enumerate(queries['block3'], 1):
            md_lines.append(f"### ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ {i + 20}")
            md_lines.append("")
            md_lines.append(f"**â“ Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ:**")
            md_lines.append("")
            md_lines.append(f"> {query}")
            md_lines.append("")

            query_result = raw_results[i-1] if i-1 < len(raw_results) else {}

            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· 'content' (Ğ½Ğµ snippet!)
            full_answer = query_result.get('content', '')

            if full_answer:
                md_lines.append(f"**ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚:**")
                md_lines.append("")
                # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼ Ğ´Ğ»Ñ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸
                paragraphs = full_answer.split('\n\n')
                for para in paragraphs:
                    if para.strip():
                        md_lines.append(para.strip())
                        md_lines.append("")

                # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· 'results' (Ñ‚Ğ¾Ğ¿ 5 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹)
                if query_result.get('results'):
                    sources = [r.get('url', '') for r in query_result['results'] if r.get('url')][:5]
                    if sources:
                        md_lines.append(f"**ğŸ”— Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ ({len(sources)}):**")
                        md_lines.append("")
                        for idx, source in enumerate(sources, 1):
                            md_lines.append(f"{idx}. {source}")
                        md_lines.append("")
            else:
                md_lines.append("**ğŸ’¬ ĞÑ‚Ğ²ĞµÑ‚:** ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
                md_lines.append("")

            # Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
            md_lines.append("---")
            md_lines.append("")

        # Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
        md_lines.append("---")
        md_lines.append("")
        md_lines.append("## Ğ˜Ñ‚Ğ¾Ğ³Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ")
        md_lines.append("")
        md_lines.append(f"- **Ğ’ÑĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¾**: 27")
        md_lines.append(f"- **Ğ’ÑĞµĞ³Ğ¾ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²**: {research_results['metadata']['sources_count']}")
        md_lines.append(f"- **ĞĞ±Ñ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸**: {research_results['metadata']['total_processing_time']}s")
        md_lines.append(f"- **Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€**: {research_results['metadata']['websearch_provider']}")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append(f"*ĞÑ‚Ñ‡Ñ‘Ñ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸*  ")
        md_lines.append(f"*Ğ”Ğ°Ñ‚Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*  ")
        md_lines.append(f"*ID Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°: {research_id}*")

        return "\n".join(md_lines)

    async def _send_research_pdf_to_admin(
        self,
        research_id: str,
        anketa_id: str,
        research_results: Dict,
        queries: Dict[str, List[str]]
    ):
        """
        ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ PDF Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ğ´Ğ¼Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ñ‡Ğ°Ñ‚

        Args:
            research_id: ID Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
            anketa_id: ID Ğ°Ğ½ĞºĞµÑ‚Ñ‹
            research_results: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
            queries: Ğ’ÑĞµ 27 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (dict Ñ block1, block2, block3)
        """
        try:
            logger.info(f"ğŸ“„ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² (MD + PDF) Ğ´Ğ»Ñ research_id={research_id}")

            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ Ğ¸Ğ· Ğ‘Ğ”
            user_info = {'username': 'Unknown', 'first_name': '', 'last_name': '', 'telegram_id': 0}
            with self.db.connect() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT u.telegram_id, u.username, u.first_name, u.last_name
                    FROM users u
                    JOIN sessions s ON s.telegram_id = u.telegram_id
                    WHERE s.anketa_id = %s
                    LIMIT 1
                """, (anketa_id,))
                user_row = cursor.fetchone()

                if user_row:
                    if isinstance(user_row, tuple):
                        user_info = {
                            'telegram_id': user_row[0],
                            'username': user_row[1] or 'unknown',
                            'first_name': (user_row[2] or '').strip(),
                            'last_name': (user_row[3] or '').strip()
                        }
                    elif hasattr(user_row, '_asdict'):
                        user_dict = user_row._asdict()
                        user_info = {
                            'telegram_id': user_dict.get('telegram_id', 0),
                            'username': user_dict.get('username', 'unknown'),
                            'first_name': (user_dict.get('first_name', '') or '').strip(),
                            'last_name': (user_dict.get('last_name', '') or '').strip()
                        }

                cursor.close()

            logger.info(f"âœ… User info Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ°: {user_info['username']}")

            # 1. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ MD Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
            md_report = self._generate_research_report_md(
                research_id=research_id,
                anketa_id=anketa_id,
                research_results=research_results,
                queries=queries,
                user_info=user_info
            )

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ MD Ñ„Ğ°Ğ¹Ğ»
            md_filename = f"{research_id.replace('#', '')}.md"
            md_filepath = os.path.join(current_dir, 'reports', md_filename)
            os.makedirs(os.path.dirname(md_filepath), exist_ok=True)

            with open(md_filepath, 'w', encoding='utf-8') as f:
                f.write(md_report)

            logger.info(f"âœ… MD Ğ¾Ñ‚Ñ‡ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {md_filepath}")

            # 2. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ PDF Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°

            # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ PDF
            # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ PDF Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°
            queries_list = []
            query_id = 1

            # Ğ‘Ğ»Ğ¾Ğº 1
            for i, query in enumerate(queries['block1']):
                block1_results = research_results.get('block1_problem', {})
                raw_results = block1_results.get('raw_results', [])
                query_result = raw_results[i] if i < len(raw_results) else {}

                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞŸĞĞ›ĞĞ«Ğ™ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· 'content' (Ğ½Ğµ snippet!)
                answer = query_result.get('content', '') or "ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"

                # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ (Ñ‚Ğ¾Ğ¿ 5 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹)
                sources = []
                if query_result.get('results'):
                    sources = [r.get('url', '') for r in query_result['results'] if r.get('url')][:5]

                queries_list.append({
                    'query_id': query_id,
                    'question': query,
                    'answer': answer,
                    'sources': sources
                })
                query_id += 1

            # Ğ‘Ğ»Ğ¾Ğº 2
            for i, query in enumerate(queries['block2']):
                block2_results = research_results.get('block2_geography', {})
                raw_results = block2_results.get('raw_results', [])
                query_result = raw_results[i] if i < len(raw_results) else {}

                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞŸĞĞ›ĞĞ«Ğ™ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· 'content' (Ğ½Ğµ snippet!)
                answer = query_result.get('content', '') or "ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"

                # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ (Ñ‚Ğ¾Ğ¿ 5 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹)
                sources = []
                if query_result.get('results'):
                    sources = [r.get('url', '') for r in query_result['results'] if r.get('url')][:5]

                queries_list.append({
                    'query_id': query_id,
                    'question': query,
                    'answer': answer,
                    'sources': sources
                })
                query_id += 1

            # Ğ‘Ğ»Ğ¾Ğº 3
            for i, query in enumerate(queries['block3']):
                block3_results = research_results.get('block3_goals', {})
                raw_results = block3_results.get('raw_results', [])
                query_result = raw_results[i] if i < len(raw_results) else {}

                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞŸĞĞ›ĞĞ«Ğ™ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· 'content' (Ğ½Ğµ snippet!)
                answer = query_result.get('content', '') or "ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"

                # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ (Ñ‚Ğ¾Ğ¿ 5 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹)
                sources = []
                if query_result.get('results'):
                    sources = [r.get('url', '') for r in query_result['results'] if r.get('url')][:5]

                queries_list.append({
                    'query_id': query_id,
                    'question': query,
                    'answer': answer,
                    'sources': sources
                })
                query_id += 1

            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ PDF
            research_data = {
                'anketa_id': anketa_id,
                'research_id': research_id,
                'queries': queries_list,
                'summary': f"Ğ’ÑĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: 27. Ğ’ÑĞµĞ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²: {research_results['metadata']['sources_count']}",
                'key_findings': f"Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {research_results['metadata']['total_processing_time']}s. ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€: {research_results['metadata']['websearch_provider']}",
                'completed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ PDF Ñ‡ĞµÑ€ĞµĞ· importlib (Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ĞµĞµ)
            import importlib.util
            pdf_gen_path = os.path.join(current_dir, 'telegram-bot', 'utils', 'stage_report_generator.py')
            spec_pdf = importlib.util.spec_from_file_location("stage_report_generator", pdf_gen_path)
            pdf_gen_module = importlib.util.module_from_spec(spec_pdf)
            spec_pdf.loader.exec_module(pdf_gen_module)
            generate_stage_pdf = pdf_gen_module.generate_stage_pdf

            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ PDF
            pdf_bytes = generate_stage_pdf('research', research_data)
            logger.info(f"âœ… PDF ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½: {len(pdf_bytes)} Ğ±Ğ°Ğ¹Ñ‚")

            # 3. ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° PDF Ğ² Ğ°Ğ´Ğ¼Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ñ‡Ğ°Ñ‚ Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ caption
            # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ AdminNotifier Ñ‡ĞµÑ€ĞµĞ· importlib
            admin_notif_path = os.path.join(current_dir, 'telegram-bot', 'utils', 'admin_notifications.py')
            spec_admin = importlib.util.spec_from_file_location("admin_notifications", admin_notif_path)
            admin_module = importlib.util.module_from_spec(spec_admin)
            spec_admin.loader.exec_module(admin_module)
            AdminNotifier = admin_module.AdminNotifier

            bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
            if not bot_token:
                logger.warning("âš ï¸ TELEGRAM_BOT_TOKEN Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, PDF Ğ½Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½")
                return

            # Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ caption (ĞºĞ°Ğº Ğ² Interview Ğ¸ Audit)
            full_name = f"{user_info['first_name']} {user_info['last_name']}".strip() or user_info['username']
            sources_count = research_results['metadata']['sources_count']
            provider = research_results['metadata']['websearch_provider']
            completed_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            caption = f"""ğŸ“Š Ğ˜Ğ¡Ğ¡Ğ›Ğ•Ğ”ĞĞ’ĞĞĞ˜Ğ• Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ

ğŸ“‹ ĞĞ½ĞºĞµÑ‚Ğ°: {anketa_id}
ğŸ‘¤ ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ: {full_name} (@{user_info['username']})
ğŸ†” Telegram ID: {user_info['telegram_id']}
ğŸ“… Ğ”Ğ°Ñ‚Ğ°: {completed_at}
ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: 27
ğŸŒ Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²: {sources_count}
âš™ï¸ ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€: {provider}

PDF Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½

#research #completed"""

            notifier = AdminNotifier(bot_token)
            success = await notifier.send_stage_completion_pdf(
                stage='research',
                pdf_bytes=pdf_bytes,
                filename=f"{research_id.replace('#', '')}.pdf",  # FIXED: Nomenclature
                caption=caption,
                anketa_id=anketa_id
            )

            if success:
                logger.info(f"âœ… PDF Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ² Ğ°Ğ´Ğ¼Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ñ‡Ğ°Ñ‚: {research_id}")
            else:
                logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ PDF Ğ² Ğ°Ğ´Ğ¼Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ñ‡Ğ°Ñ‚")

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ PDF: {e}", exc_info=True)
            # ĞĞµ Ğ±Ñ€Ğ¾ÑĞ°ĞµĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ - Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° PDF Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°

    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ (ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ°)"""
        anketa_id = data.get('anketa_id')

        if not anketa_id:
            return {
                'status': 'error',
                'message': 'anketa_id Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½'
            }

        # Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´
        return asyncio.run(self.research_with_expert_prompts(anketa_id))


# Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
ResearcherAgent = ResearcherAgentV2
