# Post-Deploy Tests - Deploy #5 (Iteration 26)

**Date:** 2025-10-23
**Deploy:** Iteration 26 - Hardcoded Question #2
**Status:** üîÑ IN PROGRESS

---

## Test Strategy

### Manual Test (Quick, Recommended First)
- **Time:** 5-10 minutes
- **Method:** Test in Telegram bot directly
- **Verifies:** Question #2 instant response, full interview flow

### Automated E2E Test (Comprehensive)
- **Time:** ~2 minutes runtime
- **Method:** Run `test_real_anketa_e2e.py` on production server
- **Verifies:** All 11 questions, data collection, audit score

---

## Manual Test Instructions

### Prerequisites:
- Telegram app open
- Bot: @grant_service_bot
- User ID: [Your Telegram ID]

### Test Scenario:

#### Step 1: Start Interview
```
User: /start
```

**Expected Response:**
```
[–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –∏ –º–µ–Ω—é —Å –∫–Ω–æ–ø–∫–∞–º–∏]
```

**‚úÖ Pass if:** Bot responds with menu

---

#### Step 2: Choose Interactive Interview
```
User: Click "üÜï –ò–Ω—Ç–µ—Ä–≤—å—é V2 (Adaptive)"
```

**Expected Response:**
```
–ö–∞–∫ –≤–∞—à–µ –∏–º—è?
```

**‚úÖ Pass if:** Bot immediately asks for name

---

#### Step 3: Answer Question #1 (Name)
```
User: –ê–Ω–¥—Ä–µ–π
```

**Expected Response:** (INSTANT - <0.1s!)
```
–ê–Ω–¥—Ä–µ–π, —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ —Å—É—Ç–∏ –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞...
```

**‚úÖ CRITICAL:** Question #2 –¥–æ–ª–∂–µ–Ω –ø—Ä–∏–π—Ç–∏ **–ú–ì–ù–û–í–ï–ù–ù–û** (<0.1s)
**‚úÖ CRITICAL:** Question #2 –¥–æ–ª–∂–µ–Ω –æ–±—Ä–∞—â–∞—Ç—å—Å—è –ø–æ –∏–º–µ–Ω–∏ ("–ê–Ω–¥—Ä–µ–π,...")

**‚ùå Fail if:**
- –û–∂–∏–¥–∞–Ω–∏–µ > 1 —Å–µ–∫—É–Ω–¥–∞
- –í–æ–ø—Ä–æ—Å –Ω–µ –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –ø–æ –∏–º–µ–Ω–∏
- –ë–æ—Ç –∑–∞–≤–∏—Å –∏–ª–∏ –≤—ã–¥–∞–ª –æ—à–∏–±–∫—É

---

#### Step 4: Answer Question #2 (Project Essence)
```
User: –°–µ—Ç—å –∫–ª—É–±–æ–≤ —Å—Ç—Ä–µ–ª—å–±—ã –∏–∑ –ª—É–∫–∞ –≤ –ú–æ—Å–∫–≤–µ
```

**Expected Response:**
```
[Question #3 - generated by LLM, might take 3-8 seconds]
–ù–∞–ø—Ä–∏–º–µ—Ä: "–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç –≤–∞—à –ø—Ä–æ–µ–∫—Ç?"
```

**‚úÖ Pass if:** Bot asks next question (3-10s normal)

---

#### Step 5: Complete Full Interview
Continue answering questions until interview completes (~8-12 questions total)

**Expected Final Response:**
```
[EXCELLENT] –ò–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ!
–û—Ü–µ–Ω–∫–∞: X.X/10
–ó–∞–¥–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: X
```

**‚úÖ Pass if:**
- Interview completes without errors
- Audit score generated (>7.0 typical)
- All answers saved

---

### Manual Test Checklist:

- [ ] Bot responds to /start
- [ ] Question #1 (name) asked immediately
- [ ] Question #2 (essence) **INSTANT** after name (<0.1s) ‚≠ê
- [ ] Question #2 uses user's name ‚≠ê
- [ ] Questions #3+ generated normally (3-10s each)
- [ ] Interview completes successfully
- [ ] Audit score generated
- [ ] No errors in bot responses
- [ ] Data saved to database

---

## Automated E2E Test on Production

### Option A: Run Test Locally Against Production Bot

**Not recommended:** Requires Telegram credentials and complex setup

### Option B: Run Test on Production Server

#### Prerequisites:
- SSH access to production server
- Test file deployed: `tests/integration/test_real_anketa_e2e.py`
- pytest installed on production

#### Steps:

**Step 1: SSH to Production**
```bash
ssh -i "C:\Users\–ê–Ω–¥—Ä–µ–π\.ssh\id_rsa" -o StrictHostKeyChecking=no root@5.35.88.251
```

**Step 2: Navigate to GrantService**
```bash
cd /var/GrantService
```

**Step 3: Check Test File Exists**
```bash
ls -l tests/integration/test_real_anketa_e2e.py
```

**Expected:**
```
-rw-r--r-- 1 root root 391 Oct 23 01:55 tests/integration/test_real_anketa_e2e.py
```

**Step 4: Run E2E Test**
```bash
cd /var/GrantService
pytest tests/integration/test_real_anketa_e2e.py -v -s
```

**Expected Output:**
```
tests/integration/test_real_anketa_e2e.py::test_real_anketa_interview_e2e

[CALLBACK] Skipped sending (hardcoded RP) - Question #2
[CALLBACK] Sent question #1
[CALLBACK] Sent question #2
... [more callback logs]

[OK] Interview completed in 108.22 seconds
[OK] Questions sent: 10
[OK] Fields collected: 11
[OK] Audit score: 8.46/10
[FAST] Question #2 instant: <0.1s

PASSED [100%]

===== 1 passed in 108.22s =====
```

**Success Criteria:**
- ‚úÖ Test PASSED
- ‚úÖ Duration: 90-120 seconds
- ‚úÖ Questions sent: 10
- ‚úÖ Fields collected: ‚â•11
- ‚úÖ Audit score: ‚â•7.0
- ‚úÖ Question #2 instant: <0.1s

---

### Step 5: Run Integration Tests (All Iteration 26 Tests)
```bash
cd /var/GrantService
pytest tests/test_iteration_26_hardcoded_question2.py -v
```

**Expected Output:**
```
tests/test_iteration_26_hardcoded_question2.py::test_hardcoded_question_instant PASSED
tests/test_iteration_26_hardcoded_question2.py::test_callback_with_none PASSED
tests/test_iteration_26_hardcoded_question2.py::test_hardcoded_skips_llm_generation PASSED
tests/test_iteration_26_hardcoded_question2.py::test_name_included_in_question PASSED
tests/test_iteration_26_hardcoded_question2.py::test_rp_001_marked_completed PASSED
tests/test_iteration_26_hardcoded_question2.py::test_no_regression_other_questions PASSED

===== 6 passed in 15.23s =====
```

**Success Criteria:**
- ‚úÖ All 6 tests PASSED
- ‚úÖ No errors or warnings

---

## Performance Verification

### Before Iteration 26:
- Question #1 (name): 0s (hardcoded)
- **Question #2 (essence): ~9.67s** (LLM generation)
- Questions #3-11: ~8s each

### After Iteration 26 (Expected):
- Question #1 (name): 0s (hardcoded)
- **Question #2 (essence): <0.1s** (hardcoded) ‚≠ê
- Questions #3-11: ~8s each

### How to Measure:

**Manual Timing:**
1. Open stopwatch on phone
2. Send answer to Question #1 (name)
3. Start stopwatch
4. When Question #2 arrives, stop stopwatch
5. **Expected: <0.1 seconds**

**Automated Timing:** (in test logs)
```
[FAST] Question #2 instant: 0.05s ‚úÖ
```

---

## Database Verification

### Check Interview Data Saved

**Step 1: SSH to Production**
```bash
ssh -i "C:\Users\–ê–Ω–¥—Ä–µ–π\.ssh\id_rsa" -o StrictHostKeyChecking=no root@5.35.88.251
```

**Step 2: Connect to PostgreSQL**
```bash
psql -h localhost -p 5434 -U grantservice -d grantservice
```

**Step 3: Query Latest Interview**
```sql
SELECT
    id,
    user_id,
    created_at,
    status,
    (questions_data::jsonb) ->> 'audit_score' as audit_score,
    jsonb_array_length(questions_data::jsonb -> 'collected_fields') as fields_count
FROM interviews
ORDER BY created_at DESC
LIMIT 1;
```

**Expected Result:**
```
 id | user_id | created_at | status | audit_score | fields_count
----|---------|------------|--------|-------------|-------------
 XX | XXXXXX  | 2025-10-23 | complete | 8.46 | 11
```

**‚úÖ Success Criteria:**
- Status: `complete`
- Audit score: ‚â•7.0
- Fields count: ‚â•10

---

## Smoke Tests

### Test 1: Bot Responds ‚úÖ
```
User: /start
Expected: Bot menu appears
```

### Test 2: Interview Starts ‚úÖ
```
User: Click "üÜï –ò–Ω—Ç–µ—Ä–≤—å—é V2"
Expected: "–ö–∞–∫ –≤–∞—à–µ –∏–º—è?"
```

### Test 3: Question #2 Instant ‚≠ê
```
User: [Your name]
Expected: Instant response (<0.1s) with project question
```

### Test 4: Interview Completes ‚úÖ
```
User: [Answer 8-12 questions]
Expected: Audit score and summary
```

---

## Regression Tests

### Verify No Breakage:

- [ ] V1 interview still works (if enabled)
- [ ] Question generation (Q#3+) still uses LLM
- [ ] Qdrant search still works
- [ ] Fallback questions available
- [ ] Audit scoring works
- [ ] Database saving works
- [ ] PDF export works (if tested)

---

## Test Results Template

### Manual Test Result:

**Date:** 2025-10-23
**Tester:** [Your name]
**Bot:** @grant_service_bot
**Test Duration:** XX minutes

**Results:**
- [ ] Question #1 immediate: [PASS/FAIL]
- [ ] Question #2 instant (<0.1s): [PASS/FAIL] ‚≠ê
- [ ] Question #2 uses name: [PASS/FAIL] ‚≠ê
- [ ] Interview completed: [PASS/FAIL]
- [ ] Audit score generated: [PASS/FAIL]
- [ ] No errors: [PASS/FAIL]

**Performance:**
- Question #2 latency: ____ seconds (target: <0.1s)
- Total interview time: ____ seconds
- Questions asked: ____
- Fields collected: ____
- Audit score: ____ / 10

**Issues Found:**
```
[List any issues]
```

**Overall Status:** [PASS/FAIL]

---

### Automated Test Result:

**Date:** 2025-10-23
**Test File:** `tests/integration/test_real_anketa_e2e.py`
**Environment:** Production Server (5.35.88.251)

**Results:**
```
[Paste pytest output here]
```

**Summary:**
- Tests run: X
- Tests passed: X
- Tests failed: X
- Duration: XX seconds

**Overall Status:** [PASS/FAIL]

---

## Next Steps After Testing

### If All Tests Pass ‚úÖ:
1. Mark deployment as successful
2. Create deployment report
3. Monitor for 1 hour
4. Update DEPLOYMENT_INDEX.md
5. Celebrate! üéâ

### If Tests Fail ‚ùå:
1. Document failures
2. Check logs for errors
3. Consider rollback if critical
4. Fix issues and redeploy

---

**Status:** üîÑ READY TO TEST
**Recommendation:** Start with Manual Test, then run Automated E2E if time permits

---

**Created:** 2025-10-23
**Test Plan Ready:** YES ‚úÖ
