#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simplified REAL LLM Dialogue Test
=================================

–ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ LLM –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∞–∫—Ü–∏–∏ –Ω–∞ –Ω–µ–ª–æ–≥–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –¥–∏–∞–ª–æ–≥ –≤ JSON —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.

–ë–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π - –ø—Ä–æ—Å—Ç–æ LLM + dialogue recording.

Author: Grant Service Testing Team
Created: 2025-10-23
Version: 1.0 (Simplified)
"""

import sys
import os
from pathlib import Path
import json
from datetime import datetime
import time

# Setup path –¥–ª—è LLM client
_grant_service = Path("C:/SnowWhiteAI/GrantService")
sys.path.insert(0, str(_grant_service))
sys.path.insert(0, str(_grant_service / "shared"))

try:
    from llm.unified_llm_client import UnifiedLLMClient
    LLM_AVAILABLE = True
except:
    print("‚ö†Ô∏è UnifiedLLMClient –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º mock LLM")
    LLM_AVAILABLE = False


# =============================================================================
# LLM Interviewer Simulator
# =============================================================================

class SimpleLLMInterviewer:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä —Å –ø—Ä—è–º—ã–º–∏ LLM –≤—ã–∑–æ–≤–∞–º–∏"""

    def __init__(self, llm_provider='claude_code'):
        self.llm_provider = llm_provider
        if LLM_AVAILABLE:
            self.llm = UnifiedLLMClient(provider=llm_provider)
        else:
            self.llm = None

        self.dialogue_history = []

    def ask_question_with_context(self, user_answer: str, collected_info: dict) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Args:
            user_answer: –û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ª–æ–≥–∏—á–Ω—ã–º!)
            collected_info: –°–æ–±—Ä–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–æ —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞

        Returns:
            str: –°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –æ—Ç –∞–≥–µ–Ω—Ç–∞
        """

        # –§–æ—Ä–º–∏—Ä—É–µ–º prompt –¥–ª—è LLM
        system_prompt = """–¢—ã - –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä –¥–ª—è –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏ –≤ –§–æ–Ω–¥ –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç—Å–∫–∏—Ö –ì—Ä–∞–Ω—Ç–æ–≤ (–§–ü–ì).

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –ó–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã —á—Ç–æ–±—ã —Å–æ–±—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∑–∞—è–≤–∫–∏
2. –ü–û–ú–û–ì–ê–¢–¨ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –¥–∞–µ—Ç –Ω–µ–ª–æ–≥–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
3. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–ª–æ—Ö–æ–π - –ø–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Å–ø—Ä–æ—Å–∏—Ç—å –∏–ª–∏ –¥–∞—Ç—å –ø—Ä–∏–º–µ—Ä
4. –ù–µ –∫—Ä–∏—Ç–∏–∫—É–π, –∞ –Ω–∞–ø—Ä–∞–≤–ª—è–π

–ö–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –¥–ª—è —Å–±–æ—Ä–∞:
- –ò–º—è
- –°—É—Ç—å –ø—Ä–æ–µ–∫—Ç–∞
- –°–æ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞
- –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
- –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
- –ë—é–¥–∂–µ—Ç
- –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_str = "\n".join([f"- {k}: {v}" for k, v in collected_info.items()])

        user_prompt = f"""–°–æ–±—Ä–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
{context_str if context_str else "(–ü–æ–∫–∞ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–æ–±—Ä–∞–Ω–æ)"}

–ü–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_answer}"

–ó–ê–î–ê–ß–ê: –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –°–õ–ï–î–£–Æ–©–ò–ô –≤–æ–ø—Ä–æ—Å –¥–ª—è –∏–Ω—Ç–µ—Ä–≤—å—é.

–ü—Ä–∞–≤–∏–ª–∞:
- –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–ª–æ—Ö–æ–π (–∫–æ—Ä–æ—Ç–∫–∏–π/–Ω–µ–ª–æ–≥–∏—á–Ω—ã–π) - –ø–æ–ø—Ä–æ–±—É–π —É—Ç–æ—á–Ω–∏—Ç—å –∏–ª–∏ –¥–∞—Ç—å –ø—Ä–∏–º–µ—Ä
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–±—Ä–∞–Ω–∞ - –ø–µ—Ä–µ—Ö–æ–¥–∏ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –ø–æ–ª—é
- –ë—É–¥—å helpful, –Ω–µ –∫—Ä–∏—Ç–∏–∫—É–π
- –û–¥–∏–Ω –≤–æ–ø—Ä–æ—Å –∑–∞ —Ä–∞–∑

–û–¢–í–ï–¢ (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π):"""

        # –í—ã–∑—ã–≤–∞–µ–º LLM
        if self.llm:
            response = self.llm.generate_text(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=200,
                temperature=0.7
            )
            return response.strip()
        else:
            # Mock response –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            return f"[MOCK] –°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–≤–µ—Ç–∞ '{user_answer[:30]}...'"


# =============================================================================
# Dialogue Recorder
# =============================================================================

class DialogueRecorder:
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π –¥–∏–∞–ª–æ–≥"""

    def __init__(self):
        self.dialogue = []
        self.metadata = {
            'test_name': 'simple_llm_dialogue',
            'start_time': datetime.now().isoformat(),
            'llm_provider': None
        }

    def add_turn(self, question: str, user_answer: str, llm_response_time: float = 0):
        """–î–æ–±–∞–≤–∏—Ç—å –æ–¥–∏–Ω —Ö–æ–¥"""
        self.dialogue.append({
            'turn': len(self.dialogue) + 1,
            'question': question,
            'user_answer': user_answer,
            'llm_response_time': llm_response_time,
            'timestamp': datetime.now().isoformat()
        })

    def save_to_file(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON"""
        self.metadata['end_time'] = datetime.now().isoformat()
        self.metadata['total_turns'] = len(self.dialogue)

        output = {
            'metadata': self.metadata,
            'dialogue': self.dialogue
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"\nüìÅ –î–∏–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")

    def print_dialogue(self):
        """–í—ã–≤–µ—Å—Ç–∏ –¥–∏–∞–ª–æ–≥ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        print("\n" + "="*80)
        print("üìñ –ü–û–õ–ù–´–ô –î–ò–ê–õ–û–ì")
        print("="*80)

        for turn in self.dialogue:
            print(f"\n--- –•–û–î {turn['turn']} ---")
            print(f"‚ùì –í–æ–ø—Ä–æ—Å: {turn['question']}")
            print(f"üë§ –û—Ç–≤–µ—Ç: {turn['user_answer']}")
            print(f"‚è±Ô∏è LLM –æ—Ç–≤–µ—Ç–∏–ª –∑–∞: {turn['llm_response_time']:.2f}s")

        print("\n" + "="*80)


# =============================================================================
# Main Test
# =============================================================================

def run_chaotic_interview():
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é —Å –Ω–µ–ª–æ–≥–∏—á–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
    """

    print("\n" + "="*80)
    print("üß™ SIMPLIFIED LLM DIALOGUE TEST")
    print("="*80)
    print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∞–∫—Ü–∏–∏ LLM –Ω–∞ –Ω–µ–ª–æ–≥–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã")
    print("–î–∏–∞–ª–æ–≥ –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ JSON —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    interviewer = SimpleLLMInterviewer(llm_provider='claude_code')
    recorder = DialogueRecorder()
    recorder.metadata['llm_provider'] = 'claude_code'

    # –ù–µ–ª–æ–≥–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞
    chaotic_answers = [
        ("–°–∫–∞–∂–∏—Ç–µ –≤–∞—à–µ –∏–º—è", "–í–∞—Å—è"),  # OK
        ("–í —á–µ–º —Å—É—Ç—å –ø—Ä–æ–µ–∫—Ç–∞?", "–ú–Ω–µ –Ω—Ä–∞–≤—è—Ç—Å—è –±–∞–Ω–∞–Ω—ã –∏ —Å–∏–Ω–∏–π —Ü–≤–µ—Ç"),  # –ü–õ–û–•–û–ô
        ("–ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç?", "asdfgh"),  # GIBBERISH
        ("–ö—Ç–æ —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è?", "–í—Å–µ"),  # –°–õ–ò–®–ö–û–ú –û–ë–©–ò–ô
        ("–û–ø–∏—à–∏—Ç–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é", "???"),  # –ë–ï–°–°–ú–´–°–õ–ï–ù–ù–´–ô
        ("–ö–∞–∫–æ–π –Ω—É–∂–µ–Ω –±—é–¥–∂–µ—Ç?", "–î–∞"),  # –û–î–ù–û–°–õ–û–ñ–ù–´–ô
    ]

    collected_info = {}

    print("üé¨ –ù–ê–ß–ê–õ–û –ò–ù–¢–ï–†–í–¨–Æ\n")

    for i, (question, answer) in enumerate(chaotic_answers, 1):
        print(f"\n{'='*80}")
        print(f"–•–û–î {i}")
        print(f"{'='*80}")

        print(f"‚ùì –í–æ–ø—Ä–æ—Å: {question}")
        print(f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç: '{answer}'")

        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–∞–µ—Ç –æ—Ç–≤–µ—Ç
        if i == 1:
            collected_info['name'] = answer
        elif i == 2:
            collected_info['project_essence'] = answer
        elif i == 3:
            collected_info['social_problem'] = answer
        elif i == 4:
            collected_info['target_audience'] = answer
        elif i == 5:
            collected_info['methodology'] = answer
        elif i == 6:
            collected_info['budget'] = answer

        # LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ –ø–ª–æ—Ö–æ–π –æ—Ç–≤–µ—Ç)
        print(f"\nü§ñ LLM –¥—É–º–∞–µ—Ç...")
        start_time = time.time()

        next_question = interviewer.ask_question_with_context(answer, collected_info)

        elapsed = time.time() - start_time
        print(f"ü§ñ LLM –æ—Ç–≤–µ—Ç–∏–ª ({elapsed:.2f}s): {next_question}")

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –¥–∏–∞–ª–æ–≥
        recorder.add_turn(
            question=question,
            user_answer=answer,
            llm_response_time=elapsed
        )

        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞
        time.sleep(0.5)

    print("\n" + "="*80)
    print("‚úÖ –ò–ù–¢–ï–†–í–¨–Æ –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*80)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–∞–ª–æ–≥
    output_file = Path(__file__).parent / f"dialogue_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    recorder.save_to_file(str(output_file))

    # –í—ã–≤–æ–¥–∏–º –ø–æ–ª–Ω—ã–π –¥–∏–∞–ª–æ–≥
    recorder.print_dialogue()

    # –ê–Ω–∞–ª–∏–∑
    print("\n" + "="*80)
    print("üìä –ê–ù–ê–õ–ò–ó")
    print("="*80)

    print(f"\n–í—Å–µ–≥–æ —Ö–æ–¥–æ–≤: {len(recorder.dialogue)}")
    print(f"LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {recorder.metadata['llm_provider']}")
    print(f"\n–î–∏–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")

    print("\nüí° –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å:")
    print("  - –ö–∞–∫ LLM —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ –Ω–µ–ª–æ–≥–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã?")
    print("  - –ü–µ—Ä–µ—Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ª–∏?")
    print("  - –î–∞–µ—Ç –ª–∏ –ø—Ä–∏–º–µ—Ä—ã?")
    print("  - –ü–æ–º–æ–≥–∞–µ—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é?")

    return str(output_file)


# =============================================================================
# Entry point
# =============================================================================

if __name__ == "__main__":
    # Fix encoding for Windows console
    import sys
    if sys.platform == 'win32':
        sys.stdout.reconfigure(encoding='utf-8')

    print("\n" + "="*80)
    print("SIMPLE LLM DIALOGUE TEST - Business Logic Robustness")
    print("="*80)

    if not LLM_AVAILABLE:
        print("\n‚ö†Ô∏è WARNING: UnifiedLLMClient –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
        print("–¢–µ—Å—Ç –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mock responses")
        print("–î–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö LLM –≤—ã–∑–æ–≤–æ–≤ –Ω—É–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å import\n")

    # –ó–∞–ø—É—Å–∫
    dialogue_file = run_chaotic_interview()

    print("\n" + "="*80)
    print("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢")
    print("="*80)
    print(f"\n‚úÖ –î–∏–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {dialogue_file}")
    print("\nüìñ –ß–∏—Ç–∞–π —Ñ–∞–π–ª —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ö–ê–ö LLM —Å–ø—Ä–∞–≤–∏–ª—Å—è —Å –Ω–µ–ª–æ–≥–∏—á–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏!")
    print("="*80 + "\n")
