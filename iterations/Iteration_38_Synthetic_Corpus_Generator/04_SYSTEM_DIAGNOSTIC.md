# System Diagnostic Report: GrantService Architecture

**Date:** 2025-10-25
**Purpose:** –ü–æ–ª–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä–µ–¥ Iteration 38 testing
**Status:** üîç ANALYSIS COMPLETE

---

## üéØ EXECUTIVE SUMMARY

**–ù–∞–π–¥–µ–Ω–æ:** –£ –Ω–∞—Å –£–ñ–ï –µ—Å—Ç—å **–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ –§–ü–ì!**

### –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:

```
PostgreSQL (localhost:5432)
‚îú‚îÄ‚îÄ knowledge_sources         # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π (–¥–æ–∫—É–º–µ–Ω—Ç—ã –§–ü–ì)
‚îú‚îÄ‚îÄ knowledge_sections        # –†–∞–∑–¥–µ–ª—ã —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏, –ø—Ä–∏–º–µ—Ä–∞–º–∏
‚îî‚îÄ‚îÄ knowledge_criteria        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏

Qdrant (5.35.88.251:6333)
‚îî‚îÄ‚îÄ knowledge_sections        # –í–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    ‚îú‚îÄ‚îÄ 384-dim embeddings    # SentenceTransformers MiniLM-L12-v2
    ‚îî‚îÄ‚îÄ Semantic search       # query_knowledge()

ExpertAgent (expert_agent/expert_agent.py)
‚îú‚îÄ‚îÄ PostgreSQL integration    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
‚îú‚îÄ‚îÄ Qdrant integration        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
‚îú‚îÄ‚îÄ SentenceTransformers      # Embeddings generation
‚îî‚îÄ‚îÄ query_knowledge()         # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
```

**–í—ã–≤–æ–¥:** –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥–æ—Ç–æ–≤–∞! –ú–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è Iteration 38.

---

## üìÅ EXISTING COMPONENTS

### 1. Expert Agent (`expert_agent/expert_agent.py`)

**Status:** ‚úÖ FULLY FUNCTIONAL

**Features:**
```python
class ExpertAgent:
    def __init__(
        postgres_host="localhost",
        qdrant_host="localhost",    # ‚Üê Production: 5.35.88.251
        embedding_model="paraphrase-multilingual-MiniLM-L12-v2"
    )

    def create_embedding(text: str) -> List[float]
        # –°–æ–∑–¥–∞—ë—Ç 384-dim –≤–µ–∫—Ç–æ—Ä—ã

    def add_knowledge_section(...)
        # –î–æ–±–∞–≤–ª—è–µ—Ç –≤ PostgreSQL + Qdrant

    def query_knowledge(
        question: str,
        fund: str = "fpg",
        top_k: int = 5,
        min_score: float = 0.5
    ) -> List[Dict]:
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã —Å scores
```

**Connection Points:**
- PostgreSQL: `localhost:5432` (–ª–æ–∫–∞–ª—å–Ω–æ), `localhost:5434` (–ø—Ä–æ–¥–∞–∫—à–Ω)
- Qdrant: `localhost:6333` (–ª–æ–∫–∞–ª—å–Ω–æ), `5.35.88.251:6333` (–ø—Ä–æ–¥–∞–∫—à–Ω)
- Model: 384-dimensional embeddings (SentenceTransformers)

---

### 2. Qdrant Scripts

#### `sync_qdrant_to_prod.py`
**Purpose:** –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö localhost ‚Üí production

```python
LOCAL_HOST = "localhost"
LOCAL_PORT = 6333

PROD_HOST = "5.35.88.251"
PROD_PORT = 6333

# –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏–∏:
# - knowledge_sections
# - –õ—é–±—ã–µ –¥—Ä—É–≥–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
```

**Usage:**
```bash
python sync_qdrant_to_prod.py
```

---

#### `generate_embeddings_prod.py`
**Purpose:** –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö embeddings –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ

```python
QDRANT_HOST = "localhost"  # –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ
QDRANT_PORT = 6333
COLLECTION_NAME = "knowledge_sections"
MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"

# –ü—Ä–æ—Ü–µ—Å—Å:
# 1. Load SentenceTransformer model
# 2. Fetch all points from Qdrant
# 3. Generate embeddings –¥–ª—è –∫–∞–∂–¥–æ–≥–æ point
# 4. Update points in Qdrant
# 5. Test semantic search
```

**Features:**
- Batch processing (50 points/batch)
- Progress tracking
- Semantic search testing
- Production-ready

---

#### `generate_embeddings_localhost.py` & `generate_embeddings_local.py`
**Purpose:** –õ–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings

Similar to prod version but –¥–ª—è localhost development.

---

### 3. Database Schema

**PostgreSQL Tables:**

#### `knowledge_sources`
```sql
CREATE TABLE knowledge_sources (
    id SERIAL PRIMARY KEY,
    title VARCHAR(255),
    url TEXT,
    document_type VARCHAR(50),
    added_at TIMESTAMP DEFAULT NOW()
);
```

**–ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:**
- –ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –§–ü–ì 2024
- –¢–∏–ø–æ–≤—ã–µ –≥—Ä–∞–Ω—Ç–æ–≤—ã–µ –∑–∞—è–≤–∫–∏
- –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏

---

#### `knowledge_sections`
```sql
CREATE TABLE knowledge_sections (
    id SERIAL PRIMARY KEY,
    source_id INTEGER REFERENCES knowledge_sources(id),
    section_type VARCHAR(50),  -- 'requirement', 'example', 'tip'
    section_name VARCHAR(255),
    content TEXT,
    char_limit INTEGER,
    priority INTEGER,
    tags TEXT[],
    created_at TIMESTAMP DEFAULT NOW()
);
```

**–°–µ–∫—Ü–∏–∏:**
- –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –±—é–¥–∂–µ—Ç—É
- –ü—Ä–∏–º–µ—Ä—ã —Ü–µ–ª–µ–π –ø—Ä–æ–µ–∫—Ç–∞
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—é
- –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç–∏

---

#### `knowledge_criteria`
```sql
CREATE TABLE knowledge_criteria (
    id SERIAL PRIMARY KEY,
    fund_name VARCHAR(50),
    criterion_name VARCHAR(255),
    weight DECIMAL(3,2),
    max_score INTEGER,
    description TEXT
);
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –§–ü–ì:**
- –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã (–≤–µ—Å: 0.25, –º–∞–∫—Å: 10)
- –û–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è (–≤–µ—Å: 0.20, –º–∞–∫—Å: 10)
- –î–æ—Å—Ç–∏–∂–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–≤–µ—Å: 0.20, –º–∞–∫—Å: 10)
- –ò —Ç.–¥.

---

### 4. Qdrant Collections

#### `knowledge_sections`
```python
Vector Config:
- Size: 384 dimensions
- Distance: Cosine similarity
- Model: paraphrase-multilingual-MiniLM-L12-v2

Payload:
{
    "section_id": int,
    "section_name": str,
    "section_type": str,
    "fund_name": str,
    "priority": int,
    "char_limit": int,
    "tags": List[str]
}

Search Example:
query = "–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –±—é–¥–∂–µ—Ç—É –ø—Ä–æ–µ–∫—Ç–∞?"
results = qdrant.search(
    collection_name="knowledge_sections",
    query_vector=embedding,
    limit=5,
    score_threshold=0.5
)
```

---

## üîó INTEGRATION POINTS

### How It's Used in Production:

**1. ProductionWriter (agents/production_writer.py)**
```python
# –í–ï–†–û–Ø–¢–ù–û –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ExpertAgent –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è:
# - –¢—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Ä–∞–∑–¥–µ–ª–∞–º
- –ü—Ä–∏–º–µ—Ä–æ–≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫
# - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ —Å–∏–º–≤–æ–ª–∞–º
# - –ö—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
```

**2. AuditorAgent (agents/auditor_agent.py)**
```python
# –í–ï–†–û–Ø–¢–ù–û –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ExpertAgent –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è:
# - –ö—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏
# - –í–µ—Å–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
# - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
```

**3. InterviewerAgent (agents/interviewer_agent.py)**
```python
# –í–ï–†–û–Ø–¢–ù–û –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ExpertAgent –¥–ª—è:
# - –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
# - –í–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
# - –ü–æ–¥—Å–∫–∞–∑–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
```

---

## üí° HOW TO USE FOR ITERATION 38

### Option 1: Embeddings for Synthetic Anketas (RECOMMENDED)

**Use Case:** –•—Ä–∞–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∞–Ω–∫–µ—Ç –¥–ª—è similarity search

```python
from expert_agent.expert_agent import ExpertAgent

# Initialize
expert = ExpertAgent(
    qdrant_host="5.35.88.251",  # Production Qdrant
    qdrant_port=6333
)

# After generating synthetic anketa:
anketa_text = f"""
{anketa['project_name']}

–ü—Ä–æ–±–ª–µ–º–∞: {anketa['problem']}
–†–µ—à–µ–Ω–∏–µ: {anketa['solution']}
–¶–µ–ª–∏: {', '.join(anketa['goals'])}
"""

# Create embedding
embedding = expert.create_embedding(anketa_text)

# Store in Qdrant (new collection: "synthetic_anketas")
from qdrant_client.models import PointStruct

expert.qdrant.upsert(
    collection_name="synthetic_anketas",
    points=[
        PointStruct(
            id=anketa_id_hash,
            vector=embedding,
            payload={
                "anketa_id": anketa_id,
                "project_name": anketa['project_name'],
                "region": anketa['region'],
                "quality_target": anketa['quality_target'],
                "synthetic": True
            }
        )
    ]
)
```

**Benefits:**
- ‚úÖ Find similar anketas (avoid duplicates)
- ‚úÖ Diversity score calculation
- ‚úÖ Topic clustering
- ‚úÖ Uses existing infrastructure!
- ‚úÖ Spends Embeddings tokens (good for Sber500!)

---

### Option 2: Use GigaChat Embeddings API (ALTERNATIVE)

**Use Case:** –ü–æ—Ç—Ä–∞—Ç–∏—Ç—å Embeddings —Ç–æ–∫–µ–Ω—ã –∏–∑ GigaChat

```python
from shared.llm.unified_llm_client import UnifiedLLMClient

# Add to UnifiedLLMClient:
async def generate_embedding(self, text: str) -> List[float]:
    """Generate embedding using GigaChat Embeddings API"""
    if self.provider == 'gigachat-embeddings':
        # Use GigaChat Embeddings API
        # Costs: ~20 tokens per text
        response = await self.client.embeddings(text)
        return response.vector
```

**Token Usage:**
- 100 anketas √ó 20 Embeddings tokens = **2,000 Embeddings tokens**
- Good for Sber500 demonstration!

---

### Option 3: Hybrid Approach (BEST)

**Combine both:**

1. **Generate synthetic anketas:** GigaChat Lite (~150K tokens)
2. **Audit anketas:** GigaChat Max (~200K tokens)
3. **Create embeddings:** GigaChat Embeddings (~2K tokens)
4. **Store in Qdrant:** Using existing ExpertAgent infrastructure
5. **Find similar:** Semantic search via ExpertAgent.query_knowledge()

**Total Tokens per 100 anketas:**
- Lite: ~150,000
- Max: ~200,000
- Embeddings: ~2,000
- **Grand Total: ~352,000 tokens** ‚úÖ

---

## üéØ RECOMMENDATIONS FOR ITERATION 38

### Phase 4 (Qdrant Integration) - UPDATE:

**Status:** ‚ö†Ô∏è **NOT OPTIONAL - HIGHLY RECOMMENDED!**

**Why:**
1. ‚úÖ Infrastructure already exists (ExpertAgent + Qdrant)
2. ‚úÖ Uses Embeddings tokens (good for Sber500!)
3. ‚úÖ Enables similarity search (avoid duplicates)
4. ‚úÖ Enables diversity metrics
5. ‚úÖ Professional architecture demonstration

**Implementation Plan:**

#### Step 1: Create Qdrant Collection for Synthetic Anketas

```python
# File: agents/qdrant_synthetic_client.py

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

class SyntheticAnketaQdrantClient:
    """Qdrant client for synthetic anketas"""

    def __init__(
        self,
        host: str = "5.35.88.251",
        port: int = 6333,
        collection_name: str = "synthetic_anketas"
    ):
        self.client = QdrantClient(host=host, port=port)
        self.collection_name = collection_name

    def create_collection_if_not_exists(self):
        """Create collection for synthetic anketas (384-dim vectors)"""
        try:
            self.client.get_collection(self.collection_name)
            logger.info(f"Collection '{self.collection_name}' already exists")
        except:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=384,  # SentenceTransformers MiniLM-L12-v2
                    distance=Distance.COSINE
                )
            )
            logger.info(f"Created collection '{self.collection_name}'")

    def add_anketa(
        self,
        anketa_id: str,
        embedding: List[float],
        anketa_data: Dict
    ):
        """Add synthetic anketa to Qdrant"""
        point = PointStruct(
            id=hash(anketa_id),  # Convert string to int ID
            vector=embedding,
            payload={
                "anketa_id": anketa_id,
                "project_name": anketa_data.get("project_name"),
                "region": anketa_data.get("region"),
                "quality_target": anketa_data.get("quality_target"),
                "synthetic": True
            }
        )

        self.client.upsert(
            collection_name=self.collection_name,
            points=[point]
        )

    def find_similar(
        self,
        embedding: List[float],
        limit: int = 5,
        min_score: float = 0.9
    ) -> List[Dict]:
        """Find similar anketas"""
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=embedding,
            limit=limit,
            score_threshold=min_score
        )

        return [
            {
                "anketa_id": hit.payload["anketa_id"],
                "similarity": hit.score,
                **hit.payload
            }
            for hit in results
        ]
```

#### Step 2: Add Embedding Generation to UnifiedLLMClient

```python
# File: shared/llm/unified_llm_client.py

async def generate_embedding(self, text: str) -> List[float]:
    """
    Generate embedding using GigaChat Embeddings API

    Args:
        text: Text to embed

    Returns:
        384-dim vector

    Tokens: ~20 Embeddings tokens per call
    """
    if self.provider == 'gigachat-embeddings':
        # Use GigaChat Embeddings API
        # Endpoint: https://gigachat.devices.sberbank.ru/api/v1/embeddings
        response = await self.client.embeddings(
            model="Embeddings",
            input=[text]
        )
        return response.data[0].embedding

    else:
        # Fallback: Use SentenceTransformers (local, free)
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        embedding = model.encode(text, convert_to_tensor=False)
        return embedding.tolist()
```

#### Step 3: Update Telegram Commands

**Update `/generate_synthetic_anketa`:**
```python
# After saving anketa to DB:

# Generate embedding
anketa_text = f"{anketa['project_name']}\n{anketa['problem']}\n{anketa['solution']}"
embedding = await llm.generate_embedding(anketa_text)

# Add to Qdrant
qdrant_client = SyntheticAnketaQdrantClient()
qdrant_client.create_collection_if_not_exists()
qdrant_client.add_anketa(
    anketa_id=saved_anketa_id,
    embedding=embedding,
    anketa_data=anketa
)
```

**Add `/find_similar_anketas [anketa_id]` command:**
```python
async def find_similar_anketas(update, context):
    """Find similar anketas using semantic search"""
    anketa_id = context.args[0]

    # Get anketa
    anketa = db.get_anketa_by_id(anketa_id)

    # Generate embedding
    anketa_text = f"{anketa['project_name']}\n{anketa['problem']}\n{anketa['solution']}"
    embedding = await llm.generate_embedding(anketa_text)

    # Search
    qdrant_client = SyntheticAnketaQdrantClient()
    similar = qdrant_client.find_similar(embedding, limit=5, min_score=0.85)

    # Show results
    message = f"üîç –ü–æ—Ö–æ–∂–∏–µ –∞–Ω–∫–µ—Ç—ã –¥–ª—è {anketa_id}:\n\n"
    for s in similar:
        message += f"‚Ä¢ {s['project_name']} ({s['similarity']:.2%} –ø–æ—Ö–æ–∂–µ—Å—Ç–∏)\n"

    await update.message.reply_text(message)
```

---

## üìä TOKEN USAGE ESTIMATE (Updated)

### With Qdrant Integration:

**Per 100 Synthetic Anketas:**
```
Generation (Lite):     ~150,000 tokens
Audit (Max):           ~200,000 tokens
Embeddings:            ~2,000 tokens   ‚Üê NEW!
Qdrant Storage:        Free (vector DB)

Total:                 ~352,000 tokens/run
```

**Weekly Target (7.7M tokens):**
```
~22 runs √ó 352K = ~7.7M tokens
~2,200 synthetic anketas generated
~2,200 anketas audited
~44,000 Embeddings tokens used

Breakdown:
- Lite:   ~3.3M tokens (165% of limit)
- Max:    ~4.4M tokens (220% of limit)  ‚Üê Critical for Sber500!
- Embed:  ~44K tokens (0.88% of limit)
```

---

## üöÄ UPDATED ITERATION 38 PLAN

### Phase 4: Qdrant Integration (RECOMMENDED)

**Status:** ‚è≥ IN PROGRESS

**Tasks:**
1. [x] Research existing infrastructure (ExpertAgent, Qdrant)
2. [ ] Create `SyntheticAnketaQdrantClient` class
3. [ ] Add `generate_embedding()` to UnifiedLLMClient
4. [ ] Update `/generate_synthetic_anketa` command
5. [ ] Add `/find_similar_anketas` command
6. [ ] Test with 10 synthetic anketas
7. [ ] Verify embeddings in Qdrant
8. [ ] Test semantic search

**Estimated Time:** 2 hours

**Benefits:**
- ‚úÖ Uses Embeddings tokens (Sber500!)
- ‚úÖ Enables similarity detection
- ‚úÖ Professional architecture
- ‚úÖ Reuses existing infrastructure

---

## üéØ NEXT STEPS

### Immediate (Today):

1. **Complete Phase 4 (Qdrant Integration):**
   - Create `SyntheticAnketaQdrantClient`
   - Update commands
   - Test integration

2. **Phase 5 (Local Testing):**
   - Test `/generate_synthetic_anketa 10 medium`
   - Test `/batch_audit_anketas 10`
   - Test `/find_similar_anketas [ID]`
   - Verify token usage

3. **Git Commit:**
   - Commit Iteration 38 after successful testing
   - Update configuration docs

### This Week:

4. **Production Run:**
   - Generate 100 synthetic anketas
   - Audit 100 anketas
   - Create 100 embeddings
   - Total: ~352K tokens spent

5. **Sber500 Demonstration:**
   - Show professional token distribution
   - Demonstrate semantic search
   - Present architecture

---

## üìù CONFIGURATION MANAGEMENT

### Single Source of Truth Needed:

**Problem:** –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–∑–±—Ä–æ—Å–∞–Ω–∞ –ø–æ —Ñ–∞–π–ª–∞–º
- `expert_agent.py` - hardcoded hosts
- `sync_qdrant_to_prod.py` - hardcoded IPs
- `generate_embeddings_*.py` - hardcoded models

**Solution:** Create centralized config

**File:** `config/system_config.py`
```python
"""
Central configuration for GrantService
Updated after successful deployments
"""

class SystemConfig:
    # PostgreSQL
    POSTGRES_LOCAL_HOST = "localhost"
    POSTGRES_LOCAL_PORT = 5432
    POSTGRES_PROD_HOST = "localhost"  # –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ
    POSTGRES_PROD_PORT = 5434
    POSTGRES_DB = "grantservice"
    POSTGRES_USER = "postgres"
    POSTGRES_PASSWORD = "root"

    # Qdrant
    QDRANT_LOCAL_HOST = "localhost"
    QDRANT_LOCAL_PORT = 6333
    QDRANT_PROD_HOST = "5.35.88.251"
    QDRANT_PROD_PORT = 6333

    # Collections
    KNOWLEDGE_COLLECTION = "knowledge_sections"
    SYNTHETIC_COLLECTION = "synthetic_anketas"

    # Models
    EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_DIM = 384

    # GigaChat
    GIGACHAT_MAX_CLIENT_ID = "967330d4-e5ab-4fca-a8e8-12a7d510d249"

    # Token Limits
    GIGACHAT_MAX_LIMIT = 1987948
    GIGACHAT_PRO_LIMIT = 2000000
    GIGACHAT_LITE_LIMIT = 2000000
    GIGACHAT_EMBED_LIMIT = 5000000

    @classmethod
    def get_env(cls) -> str:
        """Get current environment (local/production)"""
        import socket
        hostname = socket.gethostname()
        return "production" if "server" in hostname else "local"

    @classmethod
    def get_postgres_config(cls) -> dict:
        """Get PostgreSQL config for current environment"""
        env = cls.get_env()
        if env == "production":
            return {
                "host": cls.POSTGRES_PROD_HOST,
                "port": cls.POSTGRES_PROD_PORT,
                "database": cls.POSTGRES_DB,
                "user": cls.POSTGRES_USER,
                "password": cls.POSTGRES_PASSWORD
            }
        else:
            return {
                "host": cls.POSTGRES_LOCAL_HOST,
                "port": cls.POSTGRES_LOCAL_PORT,
                "database": cls.POSTGRES_DB,
                "user": cls.POSTGRES_USER,
                "password": cls.POSTGRES_PASSWORD
            }

    @classmethod
    def get_qdrant_config(cls) -> dict:
        """Get Qdrant config for current environment"""
        env = cls.get_env()
        if env == "production":
            return {
                "host": cls.QDRANT_PROD_HOST,
                "port": cls.QDRANT_PROD_PORT
            }
        else:
            return {
                "host": cls.QDRANT_LOCAL_HOST,
                "port": cls.QDRANT_LOCAL_PORT
            }
```

**Usage:**
```python
from config.system_config import SystemConfig

# Automatic environment detection
qdrant_config = SystemConfig.get_qdrant_config()
qdrant = QdrantClient(**qdrant_config)
```

---

## ‚úÖ SUMMARY

### What We Found:

1. ‚úÖ **ExpertAgent** - Fully functional knowledge base system
2. ‚úÖ **Qdrant** - Production vector database (5.35.88.251:6333)
3. ‚úÖ **PostgreSQL** - Structured knowledge (sources, sections, criteria)
4. ‚úÖ **SentenceTransformers** - 384-dim embeddings
5. ‚úÖ **Sync Scripts** - Production deployment tools

### What We Can Reuse:

1. ‚úÖ Qdrant infrastructure (no new setup needed!)
2. ‚úÖ ExpertAgent patterns (create_embedding, query_knowledge)
3. ‚úÖ Embedding model (MiniLM-L12-v2, 384-dim)
4. ‚úÖ Sync scripts (deploy to production)

### What We Need to Add:

1. ‚è≥ `SyntheticAnketaQdrantClient` class
2. ‚è≥ `generate_embedding()` in UnifiedLLMClient
3. ‚è≥ `/find_similar_anketas` command
4. ‚è≥ `SystemConfig` centralized configuration
5. ‚è≥ Integration with existing commands

### Impact on Iteration 38:

**BEFORE Diagnostic:**
- Phase 4: OPTIONAL (–º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å)

**AFTER Diagnostic:**
- Phase 4: **RECOMMENDED** (infrastructure ready, easy to integrate!)
- Benefits: +2K Embeddings tokens, similarity search, professional architecture
- Time: ~2 hours (leveraging existing code)

---

**Created:** 2025-10-25
**Purpose:** System diagnostic for Iteration 38 planning
**Result:** Infrastructure ready, Phase 4 recommended
**Status:** READY FOR IMPLEMENTATION ‚úÖ
