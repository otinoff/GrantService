# Phase 4: RAG Integration Design

**Date:** 2025-10-26
**Author:** Claude Code
**Iteration:** 51 - AI Enhancement

---

## üìä Current State Analysis

### WriterAgent Structure
**Location:** `agents/writer_agent.py`

**Current Flow:**
1. `write_application_async()` - entry point
2. `_generate_application_content_async()` - generates 10 sections sequentially:
   - title (project name)
   - summary (brief description)
   - **problem** (500-1000 words) ‚Üê **RAG TARGET #1**
   - **solution** (800-1500 words) ‚Üê **RAG TARGET #2**
   - **implementation** (1000-2000 words) ‚Üê **RAG TARGET #3**
   - **budget** (500-800 words) ‚Üê **RAG TARGET #4**
   - timeline (300-500 words)
   - team (400-600 words)
   - **impact** (600-1000 words) ‚Üê **RAG TARGET #5**
   - sustainability (400-600 words)

**Current Prompt Pattern:**
```python
problem_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥—Ä–∞–Ω—Ç–æ–≤—ã–º –∑–∞—è–≤–∫–∞–º.

–ü–†–û–ï–ö–¢: {project_name}
–û–ü–ò–°–ê–ù–ò–ï: {description}

–ù–∞–ø–∏—à–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ü–†–û–ë–õ–ï–ú–´ –¥–ª—è –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏...
"""
```

**Key Issue:** No examples, no context from successful grants, purely generative.

---

## üóÑÔ∏è Available Qdrant Collections

### Collection 1: fpg_real_winners (42 vectors)
**Source:** 17 real FPG grant winners from web research

**Vector Structure:**
- 1 grant ‚Üí 4 vectors (problem, solution, kpi, budget sections)
- Each vector has metadata: title, org, year, region, amount, category, fund_name

**Quality:** 0.81+ semantic similarity (tested)

**Use Cases:**
- Retrieve similar problem descriptions
- Retrieve solution examples
- Retrieve KPI/impact examples
- Retrieve budget breakdowns

### Collection 2: fpg_requirements_gigachat (18 vectors)
**Source:** FPG evaluation criteria, methodologies, budget templates

**Vector Structure:**
- 1 vector per requirement (not split by section like Collection 1)
- Metadata: requirement_type (criterion | methodology | budget), fund_name, category

**Quality:** 0.87 semantic similarity (tested)

**Use Cases:**
- Retrieve evaluation criteria (for quality alignment)
- Retrieve research methodologies (SMART, Logic Model, etc.)
- Retrieve budget templates (standard FPG categories)

---

## üéØ RAG Strategy: Hybrid Approach

### Strategy Comparison

| Strategy | Pros | Cons | Decision |
|----------|------|------|----------|
| **Section-by-Section** | Highly relevant per section | 5 semantic searches (cost/latency) | ‚ùå Too slow |
| **Upfront (Single Query)** | Fast, 1 search | Less targeted per section | ‚ö†Ô∏è Good but not optimal |
| **Hybrid (Recommended)** | Best relevance + efficiency | Moderate complexity | ‚úÖ **CHOSEN** |

### Hybrid Approach Details

**Phase 1: Upfront Retrieval (before generation)**
- Query: User's project description
- Target: `fpg_real_winners` collection
- Retrieve: Top-3 most similar grants (all 4 sections per grant)
- Purpose: General context about similar successful projects

**Phase 2: Section-Specific Retrieval (during generation)**
- Retrieve targeted examples for key sections:
  - **Problem**: Retrieve top-2 "problem" vectors from fpg_real_winners
  - **Solution**: Retrieve top-2 "solution" vectors from fpg_real_winners
  - **Implementation**: Retrieve top-2 "methodology" vectors from fpg_requirements_gigachat
  - **Budget**: Retrieve top-2 "budget" vectors (from both collections)
  - **Impact**: Retrieve top-2 "kpi" vectors from fpg_real_winners

**Total Queries:** 1 (upfront) + 5 (section-specific) = 6 semantic searches per grant

---

## üèóÔ∏è Architecture Design

### New Module: `shared/llm/rag_retriever.py`

```python
class QdrantRAGRetriever:
    """
    RAG retriever for WriterAgent using GigaChat Embeddings + Qdrant

    Collections:
    - fpg_real_winners: Real FPG grants (problem, solution, kpi, budget)
    - fpg_requirements_gigachat: Criteria, methodologies, budget templates
    """

    def __init__(self, qdrant_client, embeddings_client):
        """Initialize with Qdrant client and GigaChat Embeddings client"""
        self.qdrant = qdrant_client
        self.embeddings = embeddings_client

    def retrieve_similar_grants(self, query_text: str, top_k: int = 3) -> List[Dict]:
        """
        Upfront retrieval: Find top-k most similar grants

        Args:
            query_text: Project description
            top_k: Number of grants to retrieve (default: 3)

        Returns:
            List of grant metadata with all sections
        """

    def retrieve_section_examples(
        self,
        section_name: str,  # "problem" | "solution" | "kpi" | "budget"
        query_text: str,
        top_k: int = 2
    ) -> List[str]:
        """
        Section-specific retrieval from fpg_real_winners

        Args:
            section_name: Section type to retrieve
            query_text: Query for semantic search
            top_k: Number of examples (default: 2)

        Returns:
            List of text examples with metadata
        """

    def retrieve_requirements(
        self,
        requirement_type: str,  # "criterion" | "methodology" | "budget"
        query_text: str,
        top_k: int = 2
    ) -> List[str]:
        """
        Retrieve requirements/methodologies/templates

        Args:
            requirement_type: Type of requirement
            query_text: Query for semantic search
            top_k: Number of requirements (default: 2)

        Returns:
            List of requirement descriptions
        """
```

### Helper Functions

```python
def format_grant_for_prompt(grant_data: Dict) -> str:
    """
    Format retrieved grant for prompt injection

    Returns formatted string like:
    ---
    –ü–†–ò–ú–ï–† –£–°–ü–ï–®–ù–û–ô –ó–ê–Ø–í–ö–ò:
    –ü—Ä–æ–µ–∫—Ç: {title}
    –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: {organization}
    –ì–æ–¥: {year}, –°—É–º–º–∞: {amount} —Ä—É–±.

    –ü—Ä–æ–±–ª–µ–º–∞: {problem}
    –†–µ—à–µ–Ω–∏–µ: {solution}
    ---
    """

def format_requirements_for_prompt(requirements: List[Dict]) -> str:
    """
    Format requirements/methodologies for prompt injection

    Returns formatted string like:
    ---
    –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ú–ï–¢–û–î–û–õ–û–ì–ò–ò:
    1. SMART-—Ü–µ–ª–∏: Specific, Measurable, Achievable, Relevant, Time-bound
       –ü—Ä–∏–º–µ—Ä: "–ü—Ä–æ–≤–µ—Å—Ç–∏ 50 –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –¥–ª—è 1000 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –¥–æ 31.12.2025"

    2. –õ–æ–≥–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å: –í—Ö–æ–¥ ‚Üí –ü—Ä–æ—Ü–µ—Å—Å ‚Üí –í—ã—Ö–æ–¥ ‚Üí –†–µ–∑—É–ª—å—Ç–∞—Ç ‚Üí –≠—Ñ—Ñ–µ–∫—Ç
    ---
    """
```

---

## üîå Integration Points in WriterAgent

### 1. Initialization (`__init__`)

```python
from shared.llm.rag_retriever import QdrantRAGRetriever

class WriterAgent:
    def __init__(self, gigachat_auth_key=None, claude_api_key=None):
        # Existing initialization...

        # NEW: Initialize RAG retriever
        try:
            from qdrant_client import QdrantClient
            from shared.llm.gigachat_embeddings_client import GigaChatEmbeddingsClient

            self.qdrant_client = QdrantClient(":memory:")  # Or production URL
            self.embeddings_client = GigaChatEmbeddingsClient()
            self.rag_retriever = QdrantRAGRetriever(
                self.qdrant_client,
                self.embeddings_client
            )
            logger.info("‚úÖ WriterAgent: RAG retriever initialized")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è WriterAgent: RAG retriever disabled - {e}")
            self.rag_retriever = None
```

### 2. Modified `_generate_application_content_async()`

**Step 0: Upfront RAG Retrieval (NEW)**
```python
# NEW: Retrieve similar successful grants for general context
similar_grants = []
if self.rag_retriever:
    try:
        similar_grants = self.rag_retriever.retrieve_similar_grants(
            query_text=description,
            top_k=3
        )
        logger.info(f"‚úÖ RAG: Retrieved {len(similar_grants)} similar grants")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è RAG: Failed to retrieve grants - {e}")

# Format similar grants for prompt injection
rag_context = ""
if similar_grants:
    rag_context = "\n\n–ü–†–ò–ú–ï–†–´ –£–°–ü–ï–®–ù–´–• –ü–†–û–ï–ö–¢–û–í:\n\n"
    for grant in similar_grants:
        rag_context += format_grant_for_prompt(grant) + "\n"
```

**Step 3: Problem Generation (MODIFIED)**
```python
# NEW: Retrieve problem examples
problem_examples = []
if self.rag_retriever:
    try:
        problem_examples = self.rag_retriever.retrieve_section_examples(
            section_name="problem",
            query_text=f"{project_name}: {description}",
            top_k=2
        )
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è RAG: Failed to retrieve problem examples - {e}")

# MODIFIED: Enhanced prompt with RAG context
problem_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥—Ä–∞–Ω—Ç–æ–≤—ã–º –∑–∞—è–≤–∫–∞–º.

–ü–†–û–ï–ö–¢: {project_name}
–û–ü–ò–°–ê–ù–ò–ï: {description}

{rag_context if similar_grants else ""}

{"–ü–†–ò–ú–ï–†–´ –û–ü–ò–°–ê–ù–ò–Ø –ü–†–û–ë–õ–ï–ú–´:" if problem_examples else ""}
{chr(10).join(f"{i+1}. {ex}" for i, ex in enumerate(problem_examples))}

–ù–∞–ø–∏—à–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ü–†–û–ë–õ–ï–ú–´ –¥–ª—è –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏ ({int(500*word_multiplier)}-{int(1000*word_multiplier)} —Å–ª–æ–≤).

–û–ø–∏—à–∏:
- –í —á—ë–º —Å—É—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –∏ –µ—ë –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å?
- –ö–æ–≥–æ –∏ –∫–∞–∫ –æ–Ω–∞ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç? (—Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è, –º–∞—Å—à—Ç–∞–±)
- –ö–∞–∫–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –µ—Å–ª–∏ –µ—ë –Ω–µ —Ä–µ—à–∏—Ç—å?
- –ü–æ—á–µ–º—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç?

–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–º–µ—Ä—ã –≤—ã—à–µ –¥–ª—è –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏—è, –Ω–æ –ù–ï –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç. –°–æ–∑–¥–∞–π –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ï –æ–ø–∏—Å–∞–Ω–∏–µ.

–°—Ç–∏–ª—å: —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π, —É–±–µ–¥–∏—Ç–µ–ª—å–Ω—ã–π, —Å —Ñ–∞–∫—Ç–∞–º–∏ –∏ —Ü–∏—Ñ—Ä–∞–º–∏."""
```

**Similarly for other sections:**
- Solution: Retrieve "solution" examples
- Implementation: Retrieve "methodology" requirements
- Budget: Retrieve "budget" templates
- Impact: Retrieve "kpi" examples

---

## üìê Prompt Engineering Strategy

### General Principles
1. **Context First**: Place RAG examples at the top of prompt
2. **Explicit Instruction**: "–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏—è, –Ω–æ –ù–ï –∫–æ–ø–∏—Ä—É–π"
3. **Maintain Originality**: Emphasize creating unique content
4. **Format Consistency**: Use clear separators (---, numbered lists)

### Example RAG Context Format

```
–ü–†–ò–ú–ï–†–´ –£–°–ü–ï–®–ù–´–• –ü–†–û–ï–ö–¢–û–í:

---
–ü–†–ò–ú–ï–† 1: –†–∞–∑–≤–∏—Ç–∏–µ –º–æ–ª–æ–¥–µ–∂–Ω–æ–≥–æ —Å–ø–æ—Ä—Ç–∞ –≤ –Ø—Ä–æ—Å–ª–∞–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏
–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: –°–ø–æ—Ä—Ç–∏–≤–Ω—ã–π —Ñ–æ–Ω–¥ –Ø—Ä–æ—Å–ª–∞–≤–∏—è
–ì–æ–¥: 2023, –°—É–º–º–∞: 2,500,000 —Ä—É–±., –ö–∞—Ç–µ–≥–æ—Ä–∏—è: –§–∏–∑–∏—á–µ—Å–∫–∞—è –∫—É–ª—å—Ç—É—Ä–∞

–ü—Ä–æ–±–ª–µ–º–∞:
–í –Ø—Ä–æ—Å–ª–∞–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥–∏ –º–æ–ª–æ–¥–µ–∂–∏ 14-25 –ª–µ—Ç.
–ü–æ –¥–∞–Ω–Ω—ã–º —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ú–∏–Ω—Å–ø–æ—Ä—Ç–∞, –ª–∏—à—å 32% –º–æ–ª–æ–¥—ã—Ö –ª—é–¥–µ–π —Ä–µ–≥—É–ª—è—Ä–Ω–æ –∑–∞–Ω–∏–º–∞—é—Ç—Å—è —Å–ø–æ—Ä—Ç–æ–º...

–†–µ—à–µ–Ω–∏–µ:
–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö –ø–ª–æ—â–∞–¥–æ–∫ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –ø–æ 5 –≤–∏–¥–∞–º —Å–ø–æ—Ä—Ç–∞...
---

–ü–†–ò–ú–ï–† 2: –ö–∏–Ω–æ–∫–ª—É–± "–ü–µ–¥–∞–≥–æ–≥–∏–∫–∞ –∫–∏–Ω–æ" –≤ —à–∫–æ–ª–∞—Ö
–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: –§–æ–Ω–¥ —Ä–∞–∑–≤–∏—Ç–∏—è –∫–∏–Ω–æ
–ì–æ–¥: 2024, –°—É–º–º–∞: 1,800,000 —Ä—É–±., –ö–∞—Ç–µ–≥–æ—Ä–∏—è: –ö—É–ª—å—Ç—É—Ä–∞

–ü—Ä–æ–±–ª–µ–º–∞:
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∫–æ–ª—å–Ω–∏–∫–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç –¥–µ—Ñ–∏—Ü–∏—Ç –º–µ–¥–∏–∞–≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ø—Ä–∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...
---

–¢–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–π –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ï –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –¥–ª—è –¢–í–û–ï–ì–û –ø—Ä–æ–µ–∫—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–º–µ—Ä—ã –≤—ã—à–µ –∫–∞–∫ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å—Ç–∏–ª—è.
```

---

## üß™ Testing Strategy

### Unit Tests
```python
# tests/test_rag_retriever.py

def test_retrieve_similar_grants():
    """Test upfront retrieval of similar grants"""
    retriever = QdrantRAGRetriever(qdrant_client, embeddings_client)

    query = "–ü—Ä–æ–µ–∫—Ç –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –º–æ–ª–æ–¥–µ–∂–Ω–æ–≥–æ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å—Å—Ç–≤–∞"
    grants = retriever.retrieve_similar_grants(query, top_k=3)

    assert len(grants) == 3
    assert "title" in grants[0]
    assert "problem" in grants[0]

def test_retrieve_section_examples():
    """Test section-specific retrieval"""
    retriever = QdrantRAGRetriever(qdrant_client, embeddings_client)

    examples = retriever.retrieve_section_examples(
        section_name="problem",
        query_text="–ú–æ–ª–æ–¥–µ–∂–Ω–∞—è –±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞",
        top_k=2
    )

    assert len(examples) == 2
    assert len(examples[0]) > 100  # Reasonable text length
```

### Integration Tests
```python
# tests/test_writer_agent_rag.py

async def test_writer_agent_with_rag():
    """Test WriterAgent generates with RAG enhancement"""
    agent = WriterAgent()

    user_answers = {
        "project_name": "–®–∫–æ–ª–∞ –º–æ–ª–æ–¥—ã—Ö —É—á–µ–Ω—ã—Ö",
        "description": "–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ–≥–æ –∫–ª—É–±–∞ –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤",
        "budget": "1,500,000 —Ä—É–±."
    }

    result = await agent.write_application_async({
        "user_answers": user_answers,
        "research_data": {},
        "selected_grant": {}
    })

    assert result["status"] == "success"
    assert "–ü–†–ò–ú–ï–†" in result.get("rag_debug", "")  # Check RAG was used
```

### A/B Testing
**Compare quality with vs without RAG:**

| Metric | Without RAG | With RAG | Target |
|--------|-------------|----------|--------|
| ReviewerAgent Score | 6.5 | ? | 7.5+ |
| Problem detail (chars) | 800 | ? | 1200+ |
| Solution specificity | Generic | ? | Concrete |
| Budget structure | Basic | ? | FPG-aligned |
| KPI measurability | Weak | ? | SMART |

**Test Dataset:** 10 different project types (education, healthcare, sports, culture, etc.)

---

## üìä Success Metrics

### Phase 4 Goals

| Metric | Current (no RAG) | Target (with RAG) | How to Measure |
|--------|------------------|-------------------|----------------|
| **Quality Score** | 6.5/10 | 7.5+/10 | ReviewerAgent scoring |
| **Problem Depth** | 800 chars | 1200+ chars | Character count |
| **Solution Specificity** | Generic | Concrete | Manual review |
| **Budget Alignment** | 60% FPG-like | 85%+ FPG-like | Template matching |
| **KPI Measurability** | 50% SMART | 80%+ SMART | SMART criteria check |
| **Generation Time** | 60s | <90s | Latency measurement |

### Quality Improvement Formula
```
Improvement = (RAG_score - Baseline_score) / Baseline_score * 100%

Target: +15% quality improvement
Acceptable: +10% improvement
Failure threshold: <5% improvement
```

---

## üöÄ Implementation Plan

### Step 1: Implement RAG Retriever ‚úÖ (Current Task)
- [ ] Create `shared/llm/rag_retriever.py`
- [ ] Implement `QdrantRAGRetriever` class
- [ ] Implement helper functions
- [ ] Unit tests for retriever

### Step 2: Integrate into WriterAgent
- [ ] Add RAG initialization in `__init__`
- [ ] Modify `_generate_application_content_async()`
- [ ] Update prompts for 5 key sections
- [ ] Add error handling (graceful degradation)

### Step 3: Testing
- [ ] Unit tests for RAG retriever
- [ ] Integration tests for WriterAgent
- [ ] Manual testing with 5 different projects
- [ ] Quality comparison (with vs without RAG)

### Step 4: A/B Comparison
- [ ] Generate 10 grants WITHOUT RAG (baseline)
- [ ] Generate same 10 grants WITH RAG (test)
- [ ] ReviewerAgent scoring for all 20 grants
- [ ] Statistical analysis (mean, median, improvement %)

### Step 5: Documentation
- [ ] Update `STATUS.md` with Phase 4 results
- [ ] Create `RAG_RESULTS.md` with metrics
- [ ] Commit to git with detailed message

---

## üõ°Ô∏è Risk Mitigation

### Risk 1: RAG Retrieval Fails
**Mitigation:** Graceful degradation - if RAG fails, use original prompts without examples

```python
if self.rag_retriever:
    try:
        examples = self.rag_retriever.retrieve_section_examples(...)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è RAG failed, continuing without examples: {e}")
        examples = []
```

### Risk 2: Token Limit Exceeded
**Mitigation:** Truncate RAG context if total prompt > 12,000 chars (GigaChat limit)

```python
if len(final_prompt) > 12000:
    logger.warning(f"‚ö†Ô∏è Prompt too long ({len(final_prompt)} chars), truncating RAG context")
    rag_context = rag_context[:5000] + "...(truncated)"
```

### Risk 3: Quality Degradation
**Mitigation:** If A/B testing shows <5% improvement, rollback RAG integration

### Risk 4: Latency Increase
**Mitigation:** If generation time exceeds 90s, reduce top_k from 3 to 2

---

## üìù Next Steps

1. **NOW:** Implement `rag_retriever.py` module
2. **THEN:** Integrate into WriterAgent
3. **FINALLY:** Run A/B testing and measure improvement

**Estimated Time:** 3-4 hours
**Token Budget:** 0 (inference only, no training)
**Expected Quality Gain:** +15-20%

---

**Document Status:** ‚úÖ DESIGN COMPLETE
**Next Document:** `RAG_RESULTS.md` (after A/B testing)
