#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç UnifiedLLMClient –¥–ª—è GrantService
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É —Å GigaChat, Perplexity –∏ Ollama
"""

import asyncio
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ –º–æ–¥—É–ª—è–º
sys.path.append('/var/GrantService/shared')

try:
    from llm.unified_llm_client import UnifiedLLMClient
    from llm.config import AGENT_CONFIGS
    print("‚úÖ UnifiedLLMClient –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ UnifiedLLMClient: {e}")
    sys.exit(1)

async def test_gigachat():
    """–¢–µ—Å—Ç GigaChat"""
    print("\nüöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º GigaChat...")
    
    try:
        async with UnifiedLLMClient(
            provider="gigachat",
            model="GigaChat",
            temperature=0.7
        ) as client:
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            is_connected = await client.check_connection_async()
            print(f"üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat: {'‚úÖ' if is_connected else '‚ùå'}")
            
            if is_connected:
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                prompt = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
                result = await client.generate_text(prompt, max_tokens=100)
                print(f"ü§ñ –û—Ç–≤–µ—Ç GigaChat: {result[:200]}...")
                return True
            else:
                print("‚ùå GigaChat –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ GigaChat: {e}")
        return False

async def test_perplexity():
    """–¢–µ—Å—Ç Perplexity"""
    print("\nüöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º Perplexity...")
    
    try:
        async with UnifiedLLMClient(
            provider="perplexity",
            model="sonar",
            temperature=0.3
        ) as client:
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            is_connected = await client.check_connection_async()
            print(f"üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Perplexity: {'‚úÖ' if is_connected else '‚ùå'}")
            
            if is_connected:
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                prompt = "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–Ω—Ç?"
                result = await client.generate_text(prompt, max_tokens=150)
                print(f"ü§ñ –û—Ç–≤–µ—Ç Perplexity: {result[:200]}...")
                return True
            else:
                print("‚ùå Perplexity –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Perplexity: {e}")
        return False

async def test_ollama():
    """–¢–µ—Å—Ç Ollama (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
    print("\nüöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º Ollama...")
    
    try:
        async with UnifiedLLMClient(
            provider="ollama",
            model="qwen2.5:3b",
            temperature=0.7
        ) as client:
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            is_connected = await client.check_connection_async()
            print(f"üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama: {'‚úÖ' if is_connected else '‚ùå'}")
            
            if is_connected:
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                prompt = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
                result = await client.generate_text(prompt, max_tokens=100)
                print(f"ü§ñ –û—Ç–≤–µ—Ç Ollama: {result[:200]}...")
                return True
            else:
                print("‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ –∑–∞–ø—É—â–µ–Ω)")
                return False
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Ollama: {e}")
        return False

async def test_agent_configs():
    """–¢–µ—Å—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –∞–≥–µ–Ω—Ç–æ–≤"""
    print("\nüîß –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤...")
    
    for agent_type, config in AGENT_CONFIGS.items():
        print(f"üìã {agent_type}: {config['provider']} - {config['model']}")
    
    return True

async def test_grant_service_prompt():
    """–¢–µ—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏"""
    print("\nüìù –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏...")
    
    try:
        async with UnifiedLLMClient(
            provider="gigachat",
            model="GigaChat",
            temperature=0.7
        ) as client:
            
            prompt = """
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â—É—é –≥—Ä–∞–Ω—Ç–æ–≤—É—é –∑–∞—è–≤–∫—É –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:
            
            –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞: "–¶–∏—Ñ—Ä–æ–≤–∏–∑–∞—Ü–∏—è —Å–µ–ª—å—Å–∫–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏"
            –û–ø–∏—Å–∞–Ω–∏–µ: –ü—Ä–æ–µ–∫—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ –≤ —Å–µ–ª—å—Å–∫–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–∞–º –∏ –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞—Å–µ–ª–µ–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤—ã–º –Ω–∞–≤—ã–∫–∞–º.
            –ë—é–¥–∂–µ—Ç: 500,000 —Ä—É–±–ª–µ–π
            –°—Ä–æ–∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏: 12 –º–µ—Å—è—Ü–µ–≤
            
            –î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ 3-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∑–∞—è–≤–∫–∏.
            """
            
            result = await client.generate_text(prompt, max_tokens=500)
            print(f"ü§ñ –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏:\n{result}")
            return True
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞—è–≤–∫–∏: {e}")
        return False

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï UNIFIED LLM CLIENT –î–õ–Ø GRANTSERVICE")
    print("=" * 60)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    await test_agent_configs()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
    results = {}
    results['gigachat'] = await test_gigachat()
    results['perplexity'] = await test_perplexity()
    results['ollama'] = await test_ollama()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏
    results['grant_prompt'] = await test_grant_service_prompt()
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 60)
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 60)
    
    for test_name, result in results.items():
        status = "‚úÖ –†–ê–ë–û–¢–ê–ï–¢" if result else "‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢"
        print(f"{test_name:15} : {status}")
    
    working_count = sum(results.values())
    total_count = len(results)
    
    print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: {working_count}/{total_count} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ")
    
    if working_count >= 2:  # GigaChat + —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –¥—Ä—É–≥–æ–π
        print("‚úÖ UnifiedLLMClient –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ GrantService!")
    else:
        print("‚ö†Ô∏è UnifiedLLMClient —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")

if __name__ == "__main__":
    asyncio.run(main())
