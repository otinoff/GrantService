# Test Engineer Agent - –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π Cradle

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**
- `C:\SnowWhiteAI\cradle\Know-How\TESTING-METHODOLOGY.md` (1,550 lines)
- `C:\SnowWhiteAI\cradle\Know-How\SELF_LEARNING_SYSTEM_DESIGN.md` (616 lines)

**–î–∞—Ç–∞:** 2025-10-30

---

## üß™ TESTING-METHODOLOGY.md ‚Üí Test Engineer Agent

### –ü—Ä–∏–Ω—Ü–∏–ø 1: "Test What You Run, Run What You Test" (Production Parity)

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
> Tests must use same code, same configurations, same database schemas as production.
> Anti-pattern: Mocking production components that could be tested with real implementations.

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Test Engineer Agent:**

‚úÖ **–ü—Ä–∏–º–µ–Ω—è–µ–º:**
- Test Engineer Agent –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **—Ç–µ –∂–µ —Å–∞–º—ã–µ production agents** (agents/*.py)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç **—Ç–æ—Ç –∂–µ PostgreSQL** (production parity schema)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç **—Ç–æ—Ç –∂–µ GigaChat-Max** (—Ç–∞ –∂–µ –º–æ–¥–µ–ª—å, —Ç–µ –∂–µ prompts)

```python
# agents/test_engineer_agent.py

from agents.interactive_interviewer_agent import InteractiveInterviewerAgent
from agents.auditor_agent_claude import AuditorAgent
from agents.researcher_agent_v2 import ResearcherAgentV2
from agents.writer_agent_v2 import WriterAgentV2
from agents.reviewer_agent import ReviewerAgent

# ‚úÖ GOOD: –ò—Å–ø–æ–ª—å–∑—É–µ–º production code
interviewer = InteractiveInterviewerAgent()

# ‚ùå BAD: Mock –¥–ª—è production agent
# interviewer = MockInterviewer()  # NEVER DO THIS
```

**–ò—Å–∫–ª—é—á–µ–Ω–∏–µ:** WebSearch API mock (ERROR #16)
- **–ü—Ä–∏—á–∏–Ω–∞:** Claude Code WebSearch timeout >60 sec –≤ production
- **–†–µ—à–µ–Ω–∏–µ:** Mock –¢–û–õ–¨–ö–û –¥–ª—è WebSearch, –ù–ï –¥–ª—è ResearcherAgent logic
- **–ö–æ–¥:**
```python
# tests/e2e/modules/researcher_module.py (lines 80-95)

if use_mock_websearch:
    # Mock —Ç–æ–ª—å–∫–æ WebSearch response, –Ω–µ –≤–µ—Å—å ResearcherAgent
    mock_sources = [
        {"title": "–†–æ—Å—Å—Ç–∞—Ç –¥–∞–Ω–Ω—ã–µ", "url": "https://rosstat.gov.ru/...", "content": "..."},
        {"title": "–ú–∏–Ω—Ç—Ä—É–¥ –†–§", "url": "https://mintrud.gov.ru/...", "content": "..."}
    ]
    # ResearcherAgent.analyze() –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞—Å—Ç–æ—è—â–∏–π GigaChat
    result = researcher.analyze(mock_sources)
else:
    # Production path: –Ω–∞—Å—Ç–æ—è—â–∏–π WebSearch + GigaChat
    result = researcher.research(query)
```

---

### –ü—Ä–∏–Ω—Ü–∏–ø 2: Single Source of Truth (Configuration Management)

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
> Use pydantic-settings for environment-driven configuration.
> Avoid duplicate settings in .env, config.py, tests/conftest.py

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Test Engineer Agent:**

‚úÖ **–ü—Ä–∏–º–µ–Ω—è–µ–º:**
- –í—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Test Engineer Agent –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ: `tester/config.py`
- Environment variables —á–µ—Ä–µ–∑ pydantic-settings

```python
# tester/config.py

from pydantic_settings import BaseSettings

class TestEngineerConfig(BaseSettings):
    """Single source of truth –¥–ª—è Test Engineer Agent"""

    # GigaChat settings
    gigachat_model: str = "GigaChat-Max"
    gigachat_temperature: float = 0.2  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–µ—Å—Ç–æ–≤
    gigachat_credentials: str = ""  # From .env

    # Knowledge Base
    knowhow_path: str = "C:/SnowWhiteAI/GrantService/knowhow"
    qdrant_url: str = "http://localhost:6333"
    qdrant_collection: str = "test_engineer_kb"

    # E2E Tests
    e2e_modules_path: str = "tests/e2e/modules"
    artifacts_path: str = "iterations/Iteration_{iter}/artifacts"

    # User Simulator
    user_quality_level: str = "intermediate"  # beginner/intermediate/expert
    user_nko_type: str = "education"  # education/youth/culture/sport

    # Memory System
    memory_db_path: str = "tester/memory/test_engineer_memory.db"

    # Reinforcement Learning
    reward_pass: float = 1.0
    reward_fail: float = -1.0
    learning_rate: float = 0.01

    class Config:
        env_file = ".env"
        env_prefix = "TEST_ENGINEER_"

# Usage:
config = TestEngineerConfig()
```

**Environment Variables (.env):**
```bash
# Test Engineer Agent config
TEST_ENGINEER_GIGACHAT_CREDENTIALS=<secret>
TEST_ENGINEER_USER_QUALITY_LEVEL=intermediate
TEST_ENGINEER_ARTIFACTS_PATH=iterations/Iteration_{iter}/artifacts
```

---

### –ü—Ä–∏–Ω—Ü–∏–ø 3: Adapted Test Pyramid

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
```
      /\
     /E2E\     10% - Slow, expensive, brittle
    /------\
   /Integr.\  20% - Medium speed, some external deps
  /----------\
 /   Unit     \ 70% - Fast, cheap, reliable
/--------------\
```

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Test Engineer Agent:**

‚ö†Ô∏è **–ò–ó–ú–ï–ù–ï–ù–ò–ï:** Test Engineer Agent —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –ù–ê E2E!

**–ü—Ä–∏—á–∏–Ω–∞:**
- Unit tests –¥–ª—è production agents —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç (pytest tests/unit/)
- Integration tests –¥–ª—è database/API —É–∂–µ –ø–æ–∫—Ä—ã—Ç—ã (pytest tests/integration/)
- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π gap:** E2E validation –ø–µ—Ä–µ–¥ deploy

**–ù–æ–≤–∞—è –ø–∏—Ä–∞–º–∏–¥–∞ –¥–ª—è GrantService:**
```
      /\
     /E2E\     30% - Test Engineer Agent (–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω!)
    /------\
   /Integr.\  20% - Existing pytest tests
  /----------\
 /   Unit     \ 50% - Existing pytest tests
/--------------\
```

**–ß—Ç–æ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç Test Engineer Agent:**
1. Full 5-agent pipeline (Interview ‚Üí Audit ‚Üí Research ‚Üí Write ‚Üí Review)
2. User journey simulation (realistic –ù–ö–û scenarios)
3. Production database state transitions
4. Artifact generation (anketa.txt, grant.txt, research.json)

**–ß—Ç–æ –ù–ï —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç:**
- Unit logic internal to agents (covered by pytest)
- Database schema migrations (manual/alembic)
- API endpoint responses (integration tests)

---

### –ü—Ä–∏–Ω—Ü–∏–ø 4: AI/LLM Testing Strategies

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
> Challenge: LLM outputs are non-deterministic
> Strategies:
> 1. Deterministic test inputs ‚Üí Assert output structure (not exact content)
> 2. Use "golden outputs" for regression testing
> 3. LLM-as-a-Judge for semantic validation

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Test Engineer Agent:**

‚úÖ **Strategy 1: Deterministic Inputs**
```python
# tester/user_simulator.py

def generate_anketa(quality_level="intermediate", seed=42):
    """Generate deterministic user answers for reproducibility"""

    # Set seed for GigaChat sampling (–µ—Å–ª–∏ API –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
    config = {"temperature": 0.1, "seed": seed}

    prompt = f"""
    –¢—ã - —Å–æ–∏—Å–∫–∞—Ç–µ–ª—å –≥—Ä–∞–Ω—Ç–∞ –¥–ª—è –ù–ö–û (–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, —Ä–∞–±–æ—Ç–∞ —Å –º–æ–ª–æ–¥–µ–∂—å—é).
    –£—Ä–æ–≤–µ–Ω—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏: {quality_level}

    –û—Ç–≤–µ—Ç—å –Ω–∞ 10 –≤–æ–ø—Ä–æ—Å–æ–≤ –∞–Ω–∫–µ—Ç—ã. –¢–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å:
    - –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ (–∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã–π —á–µ–ª–æ–≤–µ–∫)
    - –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç–∏: {quality_level}
    - –ë–µ–∑ –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ "—à–∞–±–ª–æ–Ω–Ω–æ–≥–æ" —è–∑—ã–∫–∞

    –í–æ–ø—Ä–æ—Å 1: –û–ø–∏—à–∏—Ç–µ –≤–∞—à—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é...
    """

    # GigaChat with low temperature
    response = gigachat.generate(prompt, **config)
    return response
```

**Validation - Structure, Not Content:**
```python
# tests/e2e/modules/interviewer_module.py (line 65)

# ‚úÖ GOOD: Validate structure
assert len(questions_asked) == 10, f"Expected 10 questions, got {len(questions_asked)}"
assert anketa_length >= 4000, f"Anketa too short: {anketa_length} < 4000"

# ‚ùå BAD: Validate exact content (non-deterministic!)
# assert "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ" in anketa_text  # –ú–æ–∂–µ—Ç –±—ã—Ç—å "–æ–±—É—á–µ–Ω–∏–µ", "–ø—Ä–µ–ø–æ–¥–∞–≤–∞–Ω–∏–µ" etc
```

‚úÖ **Strategy 2: Golden Outputs (Regression Testing)**
```python
# tester/regression_tests.py

GOLDEN_OUTPUTS = {
    "test_v66_fix15": {
        "anketa_id": 999999001,
        "audit_score_range": (5.0, 9.0),  # Range, not exact
        "research_sources_min": 2,
        "grant_length_min": 15000,
        "review_score_range": (6.0, 10.0)
    }
}

def validate_against_golden(test_id, actual_results):
    """Compare actual results with golden baseline"""
    golden = GOLDEN_OUTPUTS.get(test_id)

    # Range validation (flexible)
    assert golden["audit_score_range"][0] <= actual_results["audit_score"] <= golden["audit_score_range"][1]

    # Threshold validation
    assert actual_results["grant_length"] >= golden["grant_length_min"]
```

‚úÖ **Strategy 3: LLM-as-a-Judge**
```python
# tester/llm_judge.py

class LLMJudge:
    """Use GigaChat to evaluate semantic quality of outputs"""

    def evaluate_grant_quality(self, grant_text: str) -> dict:
        """Semantic evaluation of grant application"""

        criteria_prompt = """
        –û—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º (0-10):

        1. –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã (—Ü–∏—Ñ—Ä—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        2. –ß–µ—Ç–∫–æ—Å—Ç—å —Ü–µ–ª–µ–π –∏ –∑–∞–¥–∞—á
        3. –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤
        4. –û–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –±—é–¥–∂–µ—Ç–∞
        5. –ò–∑–º–µ—Ä–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        –ó–∞—è–≤–∫–∞:
        {grant_text}

        –û—Ç–≤–µ—Ç –≤ JSON:
        {{"scores": {{"relevance": X, "clarity": Y, ...}}, "total": Z}}
        """

        response = gigachat.generate(criteria_prompt.format(grant_text=grant_text))
        scores = json.loads(response)
        return scores

    def validate(self, scores: dict) -> bool:
        """Check if grant passes quality threshold"""
        return scores["total"] >= 6.0  # Same as ReviewerAgent threshold
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ E2E test:**
```python
# tests/e2e/test_grant_workflow.py (after STEP 4)

judge = LLMJudge()
scores = judge.evaluate_grant_quality(grant_text)

if not judge.validate(scores):
    print(f"‚ö†Ô∏è  WARNING: Grant quality below threshold: {scores['total']}/10")
    print(f"   Weakest criteria: {min(scores['scores'], key=scores['scores'].get)}")
```

---

### –ü—Ä–∏–Ω—Ü–∏–ø 5: Continuous Assurance Pipeline

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
```
Code Change ‚Üí Pre-commit Hooks ‚Üí CI Tests ‚Üí Deploy ‚Üí Monitoring
              (lint, type)     (unit+int)         (logs, alerts)
```

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è GrantService + Test Engineer Agent:**

```
Code Change (agents/*.py)
    ‚Üì
Pre-commit Hook
    ‚îú‚îÄ black (formatting)
    ‚îú‚îÄ mypy (type checking)
    ‚îî‚îÄ Test Engineer Agent (quick smoke test) ‚Üê NEW!
        ‚îî‚îÄ Duration: 5 minutes
        ‚îî‚îÄ Tests: Interview + Audit only (partial E2E)
    ‚Üì
Push to GitHub
    ‚Üì
GitHub Actions CI
    ‚îú‚îÄ pytest tests/unit/
    ‚îú‚îÄ pytest tests/integration/
    ‚îî‚îÄ Test Engineer Agent (full E2E) ‚Üê NEW!
        ‚îî‚îÄ Duration: 20 minutes
        ‚îî‚îÄ Tests: All 5 agents
    ‚Üì
Manual Approval (if CI passed)
    ‚Üì
Deploy to Production (5.35.88.251)
    ‚îú‚îÄ git pull
    ‚îú‚îÄ systemctl restart grant-service
    ‚îî‚îÄ Test Engineer Agent (post-deploy validation) ‚Üê NEW!
        ‚îî‚îÄ Duration: 15 minutes
        ‚îî‚îÄ Tests: Smoke test on production DB
    ‚Üì
Monitoring
    ‚îú‚îÄ Telegram bot logs
    ‚îú‚îÄ PostgreSQL query logs
    ‚îî‚îÄ Test Engineer Agent (nightly regression) ‚Üê NEW!
        ‚îî‚îÄ Schedule: 03:00 MSK daily
        ‚îî‚îÄ Tests: Full E2E with golden outputs
```

**Implementation:**

**.git/hooks/pre-commit**
```bash
#!/bin/bash
# Pre-commit hook with Test Engineer Agent

echo "Running pre-commit checks..."

# 1. Format check
black --check agents/ tests/ || exit 1

# 2. Type check
mypy agents/ || exit 1

# 3. Test Engineer Agent - Quick Smoke Test
echo "Running Test Engineer Agent (quick mode)..."
python tester/agent.py --mode quick --timeout 300

if [ $? -ne 0 ]; then
    echo "‚ùå Test Engineer Agent failed! Fix before commit."
    exit 1
fi

echo "‚úÖ Pre-commit checks passed!"
```

**.github/workflows/ci.yml**
```yaml
name: CI with Test Engineer Agent

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Unit Tests
        run: pytest tests/unit/ -v

      - name: Integration Tests
        run: pytest tests/integration/ -v

      - name: Test Engineer Agent - Full E2E
        run: |
          python tester/agent.py \
            --mode full \
            --timeout 1200 \
            --report artifacts/test_report.md
        env:
          TEST_ENGINEER_GIGACHAT_CREDENTIALS: ${{ secrets.GIGACHAT_TOKEN }}

      - name: Upload Test Report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-engineer-report
          path: artifacts/test_report.md
```

---

## üß† SELF_LEARNING_SYSTEM_DESIGN.md ‚Üí Test Engineer Agent

### Component 1: Memory System

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
> Four types of memory:
> 1. Short-term: Current episode (conversation, session)
> 2. Long-term: Historical knowledge (facts, patterns)
> 3. Working: Active context (current task + relevant history)
> 4. Meta-memory: "Knowledge about knowledge" (what I know, what I don't)

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Test Engineer Agent:**

```python
# tester/memory.py

import sqlite3
from datetime import datetime
from typing import List, Dict

class TestEngineerMemory:
    """Memory system for Test Engineer Agent"""

    def __init__(self, db_path="tester/memory/test_engineer_memory.db"):
        self.db = sqlite3.connect(db_path)
        self._init_schema()

    def _init_schema(self):
        """Create memory tables"""
        self.db.execute("""
            CREATE TABLE IF NOT EXISTS short_term_memory (
                id INTEGER PRIMARY KEY,
                test_run_id TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                step_name TEXT,  -- Interview, Audit, Research, Write, Review
                status TEXT,     -- success, failed, warning
                details JSON,    -- validation results, error messages
                duration_sec REAL
            )
        """)

        self.db.execute("""
            CREATE TABLE IF NOT EXISTS long_term_memory (
                id INTEGER PRIMARY KEY,
                pattern_name TEXT UNIQUE,  -- "FIX_15_writer_grant_length"
                discovered_at DATETIME,
                description TEXT,
                occurrence_count INTEGER DEFAULT 1,
                last_seen DATETIME,
                fix_implemented BOOLEAN DEFAULT 0,
                related_files TEXT  -- JSON array of file paths
            )
        """)

        self.db.execute("""
            CREATE TABLE IF NOT EXISTS working_memory (
                id INTEGER PRIMARY KEY,
                context_type TEXT,  -- "current_test", "relevant_bugs", "code_snippet"
                content TEXT,
                embedding BLOB,  -- Vector embedding for RAG retrieval
                added_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)

        self.db.execute("""
            CREATE TABLE IF NOT EXISTS meta_memory (
                id INTEGER PRIMARY KEY,
                knowledge_type TEXT,  -- "agent_behavior", "database_schema", "api_contracts"
                entity_name TEXT,     -- "WriterAgent", "grants_table", "GigaChat_API"
                confidence_level REAL, -- 0.0 to 1.0
                last_validated DATETIME,
                notes TEXT
            )
        """)

        self.db.commit()

    # SHORT-TERM MEMORY: Current test run
    def store_step_result(self, test_run_id: str, step_name: str, status: str, details: dict, duration: float):
        """Store —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞ E2E —Ç–µ—Å—Ç–∞"""
        self.db.execute("""
            INSERT INTO short_term_memory (test_run_id, step_name, status, details, duration_sec)
            VALUES (?, ?, ?, ?, ?)
        """, (test_run_id, step_name, status, json.dumps(details), duration))
        self.db.commit()

    def get_current_test_context(self, test_run_id: str) -> List[Dict]:
        """Retrieve –≤—Å–µ —à–∞–≥–∏ —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ—Å—Ç–∞ –¥–ª—è analysis"""
        cursor = self.db.execute("""
            SELECT step_name, status, details, duration_sec
            FROM short_term_memory
            WHERE test_run_id = ?
            ORDER BY timestamp
        """, (test_run_id,))
        return [
            {"step": row[0], "status": row[1], "details": json.loads(row[2]), "duration": row[3]}
            for row in cursor.fetchall()
        ]

    # LONG-TERM MEMORY: Historical patterns
    def record_bug_pattern(self, pattern_name: str, description: str, related_files: List[str]):
        """Record –Ω–æ–≤—ã–π bug pattern –¥–ª—è future reference"""
        self.db.execute("""
            INSERT OR REPLACE INTO long_term_memory
            (pattern_name, discovered_at, description, occurrence_count, last_seen, related_files)
            VALUES (?, ?, ?,
                    COALESCE((SELECT occurrence_count + 1 FROM long_term_memory WHERE pattern_name = ?), 1),
                    ?, ?)
        """, (
            pattern_name,
            datetime.now(),
            description,
            pattern_name,  # For COALESCE subquery
            datetime.now(),
            json.dumps(related_files)
        ))
        self.db.commit()

    def recall_similar_bugs(self, error_message: str, limit=5) -> List[Dict]:
        """RAG-style retrieval: find similar bugs from history"""
        # Simplified: Full-text search (–≤ production: vector similarity)
        cursor = self.db.execute("""
            SELECT pattern_name, description, occurrence_count, fix_implemented
            FROM long_term_memory
            WHERE description LIKE ?
            ORDER BY occurrence_count DESC
            LIMIT ?
        """, (f"%{error_message}%", limit))

        return [
            {"pattern": row[0], "description": row[1], "count": row[2], "fixed": bool(row[3])}
            for row in cursor.fetchall()
        ]

    # WORKING MEMORY: Active context
    def load_working_context(self, test_run_id: str) -> Dict:
        """Assemble working memory –¥–ª—è LLM context"""
        return {
            "current_test": self.get_current_test_context(test_run_id),
            "relevant_bugs": self.recall_similar_bugs(""),  # –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –Ω–µ–¥–∞–≤–Ω–∏–µ
            "code_snippets": self._retrieve_code_context()
        }

    def _retrieve_code_context(self) -> List[str]:
        """RAG: Retrieve relevant code snippets from knowhow/"""
        # Implement RAG retrieval here (Qdrant integration)
        pass

    # META-MEMORY: Knowledge about knowledge
    def update_confidence(self, entity_name: str, confidence: float, notes: str):
        """Update confidence –≤ –∑–Ω–∞–Ω–∏–∏ –æ–± entity (agent, table, API)"""
        self.db.execute("""
            INSERT OR REPLACE INTO meta_memory
            (knowledge_type, entity_name, confidence_level, last_validated, notes)
            VALUES (?, ?, ?, ?, ?)
        """, ("agent_behavior", entity_name, confidence, datetime.now(), notes))
        self.db.commit()

    def get_confidence(self, entity_name: str) -> float:
        """Check confidence level –¥–ª—è decision making"""
        cursor = self.db.execute("""
            SELECT confidence_level FROM meta_memory
            WHERE entity_name = ?
        """, (entity_name,))
        row = cursor.fetchone()
        return row[0] if row else 0.5  # Default: uncertain
```

**Usage Example:**
```python
# tester/agent.py

memory = TestEngineerMemory()

# Test run starts
test_run_id = "test_" + datetime.now().strftime("%Y%m%d_%H%M%S")

# STEP 1: Interview
start = time.time()
result = interview_module.run_interview(user_answers)
duration = time.time() - start

memory.store_step_result(
    test_run_id=test_run_id,
    step_name="Interview",
    status="success" if result["status"] == "ok" else "failed",
    details={"anketa_id": result["anketa_id"], "length": result["length"]},
    duration=duration
)

# STEP 4: Writer (FIX #15)
result = writer_module.run_writer(anketa_id)

if result["grant_length"] < 15000:
    # Record bug pattern in long-term memory
    memory.record_bug_pattern(
        pattern_name="writer_grant_length_short",
        description=f"WriterModule returned grant_length={result['grant_length']} < 15000",
        related_files=["tests/e2e/modules/writer_module.py", "agents/writer_agent_v2.py"]
    )

    # Check if we've seen this before
    similar_bugs = memory.recall_similar_bugs("grant_length")
    if similar_bugs:
        print(f"‚ö†Ô∏è  Similar bug seen {similar_bugs[0]['count']} times before!")
        print(f"   Pattern: {similar_bugs[0]['pattern']}")

# Working memory –¥–ª—è LLM analysis
working_context = memory.load_working_context(test_run_id)

# GigaChat analysis with context
analysis_prompt = f"""
Analyze test run results:

Current test: {json.dumps(working_context['current_test'], indent=2)}
Similar historical bugs: {json.dumps(working_context['relevant_bugs'], indent=2)}

Question: Should we proceed to STEP 5 (Review) or investigate Writer failure?
"""

decision = gigachat.generate(analysis_prompt)
```

---

### Component 2: Feedback Loop System (Reinforcement Learning)

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
> Reinforcement Learning components:
> - State: System state representation
> - Action: Agent's decision/action
> - Reward: +/- based on outcome
> - Policy: Strategy for action selection
> - Value Function: Expected long-term reward

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Test Engineer Agent:**

```python
# tester/reinforcement_learning.py

import numpy as np
from typing import Dict, List

class TestPolicyLearner:
    """RL system –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ test selection"""

    def __init__(self, learning_rate=0.01, discount_factor=0.9):
        self.lr = learning_rate
        self.gamma = discount_factor

        # Q-table: state -> action -> expected reward
        # State: (last_commit_files, time_since_deploy)
        # Action: (full_e2e, quick_smoke, skip_test)
        self.q_table = {}

    def get_state(self, commit_files: List[str], hours_since_deploy: int) -> str:
        """Represent current state as hashable string"""
        # Categorize files
        file_categories = set()
        for f in commit_files:
            if f.startswith("agents/"):
                file_categories.add("agent_code")
            elif f.startswith("tests/"):
                file_categories.add("test_code")
            elif f.startswith("data/database/"):
                file_categories.add("database")

        # Discretize time
        time_bucket = "recent" if hours_since_deploy < 24 else "old"

        return f"files:{','.join(sorted(file_categories))}_time:{time_bucket}"

    def select_action(self, state: str, epsilon=0.1) -> str:
        """Epsilon-greedy action selection"""
        actions = ["full_e2e", "quick_smoke", "skip_test"]

        # Exploration (10% chance)
        if np.random.random() < epsilon:
            return np.random.choice(actions)

        # Exploitation: choose best action
        if state not in self.q_table:
            self.q_table[state] = {a: 0.0 for a in actions}

        q_values = self.q_table[state]
        return max(q_values, key=q_values.get)

    def update_policy(self, state: str, action: str, reward: float, next_state: str):
        """Q-learning update"""
        if state not in self.q_table:
            self.q_table[state] = {a: 0.0 for a in ["full_e2e", "quick_smoke", "skip_test"]}
        if next_state not in self.q_table:
            self.q_table[next_state] = {a: 0.0 for a in ["full_e2e", "quick_smoke", "skip_test"]}

        # Q(s, a) ‚Üê Q(s, a) + Œ±[r + Œ≥ * max_a' Q(s', a') - Q(s, a)]
        current_q = self.q_table[state][action]
        max_next_q = max(self.q_table[next_state].values())

        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)
        self.q_table[state][action] = new_q

    def compute_reward(self, test_result: Dict) -> float:
        """Reward function based on test outcome"""
        if test_result["status"] == "passed":
            # Positive reward
            base_reward = 1.0

            # Bonus: Fast test completion
            if test_result["duration"] < 600:  # < 10 minutes
                base_reward += 0.5

            # Bonus: No warnings
            if test_result["warnings"] == 0:
                base_reward += 0.2

            return base_reward

        elif test_result["status"] == "failed":
            # Negative reward
            base_penalty = -1.0

            # Harsher penalty: Bug reached production
            if test_result["bug_severity"] == "critical":
                base_penalty -= 2.0

            # Penalty: Wasted time on irrelevant test
            if test_result["false_positive"]:
                base_penalty -= 0.5

            return base_penalty

        else:  # skipped
            # Small penalty for not running test (missed opportunity)
            return -0.1


# Usage in Test Engineer Agent
policy_learner = TestPolicyLearner()

# Pre-commit hook decision
commit_files = ["agents/writer_agent_v2.py", "agents/auditor_agent_claude.py"]
hours_since_last_deploy = 3

state = policy_learner.get_state(commit_files, hours_since_last_deploy)
action = policy_learner.select_action(state)

print(f"RL Policy Decision: {action}")

if action == "full_e2e":
    test_result = run_full_e2e_test()
elif action == "quick_smoke":
    test_result = run_quick_smoke_test()
else:
    test_result = {"status": "skipped", "duration": 0, "warnings": 0}

# Compute reward
reward = policy_learner.compute_reward(test_result)

# Update policy
next_state = policy_learner.get_state([], hours_since_last_deploy + 1)
policy_learner.update_policy(state, action, reward, next_state)

print(f"Reward: {reward}, Q-value updated for state '{state}', action '{action}'")
```

**Reward Function Examples:**

| Scenario | Test Action | Outcome | Reward | Explanation |
|----------|-------------|---------|--------|-------------|
| Writer fix commit | Full E2E | Passed (15 min) | +1.5 | Fast, no warnings |
| Docs update | Skip test | No bug found | -0.1 | Wasted opportunity (should have tested) |
| Agent refactor | Quick smoke | Failed (bug) | -1.5 | Bug slipped through |
| Agent refactor | Full E2E | Caught bug | +2.0 | Bug prevented! |
| Database migration | Full E2E | Passed (25 min) | +1.0 | Slow but thorough |

**Over time:**
- Policy learns: "writer_agent_v2.py commits ‚Üí always full E2E"
- Policy learns: "README.md commits ‚Üí skip test"
- Policy learns: "late night commits ‚Üí full E2E (higher bug risk)"

---

### Component 3: Knowledge Graph

**–û—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ Cradle:**
> Knowledge Graph structure:
> - Nodes: Concepts, entities, facts
> - Edges: Relationships, dependencies
> - Queries: "What depends on X?", "How does Y relate to Z?"

**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Test Engineer Agent:**

```python
# tester/knowledge_graph.py

import networkx as nx
from typing import List, Dict

class TestKnowledgeGraph:
    """Knowledge graph for Test Engineer Agent"""

    def __init__(self):
        self.graph = nx.DiGraph()
        self._build_initial_graph()

    def _build_initial_graph(self):
        """Initialize graph with GrantService architecture"""

        # AGENTS (nodes)
        agents = [
            "InteractiveInterviewerAgent",
            "AuditorAgent",
            "ResearcherAgentV2",
            "WriterAgentV2",
            "ReviewerAgent"
        ]
        for agent in agents:
            self.graph.add_node(agent, type="agent", llm="GigaChat-Max")

        # DATABASE TABLES (nodes)
        tables = ["users", "sessions", "anketa", "auditor_results", "researcher_research", "grants"]
        for table in tables:
            self.graph.add_node(table, type="database_table")

        # DEPENDENCIES (edges)
        self.graph.add_edge("InteractiveInterviewerAgent", "anketa", relation="writes_to")
        self.graph.add_edge("AuditorAgent", "anketa", relation="reads_from")
        self.graph.add_edge("AuditorAgent", "auditor_results", relation="writes_to")
        self.graph.add_edge("ResearcherAgentV2", "anketa", relation="reads_from")
        self.graph.add_edge("ResearcherAgentV2", "researcher_research", relation="writes_to")
        self.graph.add_edge("WriterAgentV2", "anketa", relation="reads_from")
        self.graph.add_edge("WriterAgentV2", "researcher_research", relation="reads_from")
        self.graph.add_edge("WriterAgentV2", "grants", relation="writes_to")
        self.graph.add_edge("ReviewerAgent", "grants", relation="reads_from")

        # E2E TEST MODULES (nodes)
        test_modules = [
            "interviewer_module",
            "auditor_module",
            "researcher_module",
            "writer_module",
            "reviewer_module"
        ]
        for module in test_modules:
            self.graph.add_node(module, type="test_module")

        # TEST -> AGENT mapping (edges)
        self.graph.add_edge("interviewer_module", "InteractiveInterviewerAgent", relation="tests")
        self.graph.add_edge("auditor_module", "AuditorAgent", relation="tests")
        self.graph.add_edge("researcher_module", "ResearcherAgentV2", relation="tests")
        self.graph.add_edge("writer_module", "WriterAgentV2", relation="tests")
        self.graph.add_edge("reviewer_module", "ReviewerAgent", relation="tests")

        # BUGS/FIXES (nodes)
        self.graph.add_node("FIX_15_writer_grant_length", type="bug_fix", status="fixed")
        self.graph.add_edge("FIX_15_writer_grant_length", "writer_module", relation="affects")
        self.graph.add_edge("FIX_15_writer_grant_length", "WriterAgentV2", relation="fixed_in")

    def add_bug_fix(self, fix_name: str, affected_modules: List[str], status="open"):
        """Record –Ω–æ–≤—ã–π bug fix –≤ knowledge graph"""
        self.graph.add_node(fix_name, type="bug_fix", status=status)
        for module in affected_modules:
            self.graph.add_edge(fix_name, module, relation="affects")

    def query_impact(self, entity_name: str) -> Dict:
        """Query: What does this entity impact?"""
        if entity_name not in self.graph:
            return {"error": f"Entity '{entity_name}' not found"}

        # Find all outgoing edges
        impacts = []
        for successor in self.graph.successors(entity_name):
            edge_data = self.graph.get_edge_data(entity_name, successor)
            impacts.append({
                "target": successor,
                "relation": edge_data["relation"],
                "type": self.graph.nodes[successor].get("type")
            })

        return {"entity": entity_name, "impacts": impacts}

    def query_dependencies(self, entity_name: str) -> Dict:
        """Query: What does this entity depend on?"""
        if entity_name not in self.graph:
            return {"error": f"Entity '{entity_name}' not found"}

        # Find all incoming edges
        deps = []
        for predecessor in self.graph.predecessors(entity_name):
            edge_data = self.graph.get_edge_data(predecessor, entity_name)
            deps.append({
                "source": predecessor,
                "relation": edge_data["relation"],
                "type": self.graph.nodes[predecessor].get("type")
            })

        return {"entity": entity_name, "dependencies": deps}

    def recommend_tests(self, changed_files: List[str]) -> List[str]:
        """Recommend which E2E modules to run based on changed files"""
        tests_to_run = set()

        for file in changed_files:
            # Map file to agent
            if "writer_agent" in file:
                agent = "WriterAgentV2"
            elif "auditor_agent" in file:
                agent = "AuditorAgent"
            elif "interviewer" in file:
                agent = "InteractiveInterviewerAgent"
            elif "researcher" in file:
                agent = "ResearcherAgentV2"
            elif "reviewer" in file:
                agent = "ReviewerAgent"
            else:
                continue  # Unknown file

            # Find test module that tests this agent
            for predecessor in self.graph.predecessors(agent):
                if self.graph.nodes[predecessor].get("type") == "test_module":
                    tests_to_run.add(predecessor)

        return list(tests_to_run)


# Usage
kg = TestKnowledgeGraph()

# Query impact of WriterAgentV2
impact = kg.query_impact("WriterAgentV2")
print(f"WriterAgentV2 impacts: {impact['impacts']}")
# Output: [{"target": "grants", "relation": "writes_to", "type": "database_table"}]

# Query dependencies of writer_module test
deps = kg.query_dependencies("writer_module")
print(f"writer_module depends on: {deps['dependencies']}")

# Recommend tests for commit
changed_files = ["agents/writer_agent_v2.py", "agents/auditor_agent_claude.py"]
recommended_tests = kg.recommend_tests(changed_files)
print(f"Recommended tests: {recommended_tests}")
# Output: ["writer_module", "auditor_module"]
```

---

## üîÑ Integration: All Components Together

**Test Engineer Agent Main Loop:**

```python
# tester/agent.py

from tester.memory import TestEngineerMemory
from tester.reinforcement_learning import TestPolicyLearner
from tester.knowledge_graph import TestKnowledgeGraph
from tester.rag_retriever import RAGRetriever

class TestEngineerAgent:
    """Main agent orchestrating all components"""

    def __init__(self):
        self.memory = TestEngineerMemory()
        self.policy = TestPolicyLearner()
        self.knowledge_graph = TestKnowledgeGraph()
        self.rag = RAGRetriever(knowhow_path="knowhow/")

    def run_test_cycle(self, commit_hash: str, changed_files: List[str]):
        """Main test cycle with learning"""

        # 1. KNOWLEDGE GRAPH: Recommend tests
        recommended_tests = self.knowledge_graph.recommend_tests(changed_files)
        print(f"Knowledge Graph recommends: {recommended_tests}")

        # 2. REINFORCEMENT LEARNING: Policy decision
        state = self.policy.get_state(changed_files, hours_since_deploy=2)
        action = self.policy.select_action(state)
        print(f"RL Policy decision: {action}")

        # 3. RAG: Retrieve relevant context
        context_docs = self.rag.retrieve(query=f"How to test {changed_files[0]}?", k=3)
        print(f"RAG retrieved {len(context_docs)} relevant docs")

        # 4. MEMORY: Load working context
        test_run_id = f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        working_context = self.memory.load_working_context(test_run_id)

        # 5. RUN TEST (based on policy decision)
        if action == "full_e2e":
            test_result = self._run_full_e2e(test_run_id)
        elif action == "quick_smoke":
            test_result = self._run_quick_smoke(test_run_id)
        else:
            test_result = {"status": "skipped"}

        # 6. COMPUTE REWARD
        reward = self.policy.compute_reward(test_result)

        # 7. UPDATE POLICY
        next_state = self.policy.get_state([], hours_since_deploy=3)
        self.policy.update_policy(state, action, reward, next_state)

        # 8. UPDATE KNOWLEDGE GRAPH (if bug found)
        if test_result["status"] == "failed":
            bug_name = f"BUG_{commit_hash[:7]}_{test_result['failed_step']}"
            self.knowledge_graph.add_bug_fix(bug_name, recommended_tests, status="open")

            # 9. UPDATE MEMORY (long-term pattern)
            self.memory.record_bug_pattern(
                pattern_name=bug_name,
                description=test_result["error_message"],
                related_files=changed_files
            )

        # 10. GENERATE REPORT
        self._generate_report(test_run_id, test_result, reward, context_docs)

    def _run_full_e2e(self, test_run_id: str) -> Dict:
        """Run full 5-step E2E test"""
        # Import E2E modules
        from tests.e2e.modules import interviewer_module, auditor_module, ...

        results = {}

        # STEP 1: Interview
        start = time.time()
        interview_result = interviewer_module.run_interview(...)
        duration = time.time() - start

        self.memory.store_step_result(
            test_run_id, "Interview",
            "success" if interview_result["status"] == "ok" else "failed",
            interview_result, duration
        )

        # ... repeat for all 5 steps

        return results

    def _generate_report(self, test_run_id: str, results: Dict, reward: float, context: List):
        """Generate markdown report with all context"""
        report = f"""
# Test Engineer Agent Report

**Test Run ID:** {test_run_id}
**Timestamp:** {datetime.now()}
**RL Reward:** {reward}

## Test Results
{json.dumps(results, indent=2)}

## Retrieved Context
{chr(10).join([f"- {doc['title']}" for doc in context])}

## Memory State
- Short-term events: {len(self.memory.get_current_test_context(test_run_id))}
- Long-term patterns: {len(self.memory.db.execute("SELECT * FROM long_term_memory").fetchall())}

## Knowledge Graph Insights
- Total nodes: {self.knowledge_graph.graph.number_of_nodes()}
- Total edges: {self.knowledge_graph.graph.number_of_edges()}
        """

        with open(f"artifacts/{test_run_id}_report.md", "w") as f:
            f.write(report)
```

---

## üìä Summary

### Cradle TESTING-METHODOLOGY ‚Üí Test Engineer Agent

| –ü—Ä–∏–Ω—Ü–∏–ø Cradle | –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è GrantService |
|----------------|----------------------------|
| Production Parity | ‚úÖ Use production agents, DB, GigaChat |
| Single Source of Truth | ‚úÖ `tester/config.py` with pydantic-settings |
| Adapted Test Pyramid | ‚ö†Ô∏è Modified: 30% E2E (agent focus), 50% unit, 20% integration |
| AI/LLM Testing | ‚úÖ Deterministic inputs, golden outputs, LLM-as-Judge |
| Continuous Assurance | ‚úÖ Pre-commit hook, CI/CD, post-deploy validation |

### Cradle SELF_LEARNING ‚Üí Test Engineer Agent

| Component Cradle | Implementation |
|------------------|----------------|
| Memory System | ‚úÖ SQLite DB: short-term, long-term, working, meta-memory |
| Feedback Loop (RL) | ‚úÖ Q-learning for test policy optimization |
| Knowledge Graph | ‚úÖ NetworkX graph: agents, tables, tests, bugs |
| RAG Retrieval | ‚úÖ Qdrant embeddings –¥–ª—è knowhow/ search |

---

**Next:** Read `02_IMPLEMENTATION_PLAN.md` –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ roadmap
