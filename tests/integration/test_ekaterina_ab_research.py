#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
A/B Test: 2 –ø–æ–ª–Ω—ã—Ö researcher –ø—Ä–æ–≥–æ–Ω–∞ –¥–ª—è –∞–Ω–∫–µ—Ç—ã –ï–∫–∞—Ç–µ—Ä–∏–Ω—ã

–¶–µ–ª—å:
- –ó–∞–ø—É—Å—Ç–∏—Ç—å ResearcherAgentV2 –î–í–ê –†–ê–ó–ê –¥–ª—è –æ–¥–Ω–æ–π anketa_id
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏—è 2+ research versions –≤ –ë–î
- –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ Word –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

–ê–≤—Ç–æ—Ä: Claude Code
–î–∞—Ç–∞: 2025-10-12
"""

import sys
import os
import asyncio
from datetime import datetime
import json
from pathlib import Path

# Set PostgreSQL environment variables
os.environ['PGHOST'] = 'localhost'
os.environ['PGPORT'] = '5432'
os.environ['PGUSER'] = 'postgres'
os.environ['PGPASSWORD'] = 'root'
os.environ['PGDATABASE'] = 'grantservice'

# Set UTF-8 encoding for console output
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(__file__), 'agents'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'web-admin'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'shared'))

from data.database.models import GrantServiceDatabase
from agents.researcher_agent_v2 import ResearcherAgentV2

# –î–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Word
try:
    from docx import Document
    from docx.shared import Inches, Pt, RGBColor
    from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
    DOCX_AVAILABLE = True
except ImportError:
    print("[WARN] python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —ç–∫—Å–ø–æ—Ä—Ç –≤ Word –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    DOCX_AVAILABLE = False


async def run_ab_test():
    """
    A/B —Ç–µ—Å—Ç: –∑–∞–ø—É—Å—Ç–∏—Ç—å researcher 2 —Ä–∞–∑–∞ –¥–ª—è –æ–¥–Ω–æ–π –∞–Ω–∫–µ—Ç—ã
    """
    print("=" * 80)
    print("üß™ A/B TEST: –ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ - 2 –≤–µ—Ä—Å–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (27 queries √ó 2)")
    print("=" * 80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
    db = GrantServiceDatabase()

    # –ê–Ω–∫–µ—Ç–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω—ã
    ANKETA_ID = "EKATERINA_20251010_235448"

    print(f"\nüìã Anketa ID: {ANKETA_ID}")
    print(f"üìä –ü—Ä–æ–µ–∫—Ç: –í–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ —Ö—Ä–∞–º–∞ –∏ –Ω–∞—Ä–æ–¥–Ω—ã—Ö —Ç—Ä–∞–¥–∏—Ü–∏–π –≤ —Å–µ–ª–µ –ê–Ω–∏—Å–∏–º–æ–≤–æ")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ research
    print("\n" + "=" * 80)
    print("üîç –ü–†–û–í–ï–†–ö–ê: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    print("=" * 80)

    with db.connect() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT id, research_id, status, llm_provider, created_at,
                   CASE
                       WHEN research_results IS NULL OR research_results::text = '{}' THEN 0
                       ELSE 1
                   END as has_results
            FROM researcher_research
            WHERE anketa_id = %s
            ORDER BY created_at DESC
        """, (ANKETA_ID,))

        existing = cursor.fetchall()

        if existing:
            print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(existing)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö research:")
            for row in existing:
                id_val, research_id, status, provider, created_at, has_results = row
                print(f"   [{id_val}] {research_id} | {status} | {provider} | {created_at} | results={bool(has_results)}")
        else:
            print(f"\n‚ö†Ô∏è  –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –¥–ª—è anketa_id={ANKETA_ID} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

        cursor.close()

    # VERSION A: –ü–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω
    print("\n" + "=" * 80)
    print("üî¨ VERSION A: –ü–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω (27 queries)")
    print("=" * 80)
    print(f"‚è∞ –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%H:%M:%S')}")

    researcher_a = ResearcherAgentV2(db, llm_provider="claude_code")

    result_a = await researcher_a.research_with_expert_prompts(ANKETA_ID)

    print(f"\n‚úÖ VERSION A –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   Research ID: {result_a['research_id']}")
    print(f"   Status: {result_a['status']}")

    if result_a['status'] == 'completed':
        metadata_a = result_a['research_results']['metadata']
        print(f"   Queries: {metadata_a['total_queries']}")
        print(f"   Sources: {metadata_a['sources_count']}")
        print(f"   Time: {metadata_a['total_processing_time']}s")
        print(f"   Provider: {metadata_a.get('websearch_provider', 'unknown')}")

    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –≤—Ç–æ—Ä—ã–º –ø—Ä–æ–≥–æ–Ω–æ–º
    print("\n‚è∏Ô∏è  –ü–∞—É–∑–∞ 5 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ VERSION B...")
    await asyncio.sleep(5)

    # VERSION B: –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω
    print("\n" + "=" * 80)
    print("üî¨ VERSION B: –í—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–æ–Ω (27 queries)")
    print("=" * 80)
    print(f"‚è∞ –ù–∞—á–∞–ª–æ: {datetime.now().strftime('%H:%M:%S')}")

    researcher_b = ResearcherAgentV2(db, llm_provider="claude_code")

    result_b = await researcher_b.research_with_expert_prompts(ANKETA_ID)

    print(f"\n‚úÖ VERSION B –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   Research ID: {result_b['research_id']}")
    print(f"   Status: {result_b['status']}")

    if result_b['status'] == 'completed':
        metadata_b = result_b['research_results']['metadata']
        print(f"   Queries: {metadata_b['total_queries']}")
        print(f"   Sources: {metadata_b['sources_count']}")
        print(f"   Time: {metadata_b['total_processing_time']}s")
        print(f"   Provider: {metadata_b.get('websearch_provider', 'unknown')}")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –≤ –ë–î —Ç–µ–ø–µ—Ä—å 2 –≤–µ—Ä—Å–∏–∏
    print("\n" + "=" * 80)
    print("üîç –ü–†–û–í–ï–†–ö–ê: –û–±–µ –≤–µ—Ä—Å–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î?")
    print("=" * 80)

    with db.connect() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT id, research_id, status, llm_provider,
                   research_results->>'metadata' as metadata_json,
                   created_at, completed_at
            FROM researcher_research
            WHERE anketa_id = %s
            ORDER BY created_at DESC
            LIMIT 10
        """, (ANKETA_ID,))

        all_research = cursor.fetchall()
        cursor.close()

    print(f"\nüìä –í—Å–µ–≥–æ research –¥–ª—è anketa_id={ANKETA_ID}: {len(all_research)}")
    print("\n–î–µ—Ç–∞–ª–∏:")

    for idx, row in enumerate(all_research, 1):
        id_val, research_id, status, provider, metadata_json, created_at, completed_at = row

        print(f"\n[{idx}] {research_id}")
        print(f"    ID: {id_val}")
        print(f"    Status: {status}")
        print(f"    Provider: {provider}")
        print(f"    Created: {created_at}")
        print(f"    Completed: {completed_at}")

        if metadata_json:
            try:
                metadata = json.loads(metadata_json)
                print(f"    Queries: {metadata.get('total_queries', 'N/A')}")
                print(f"    Sources: {metadata.get('sources_count', 'N/A')}")
                print(f"    Time: {metadata.get('total_processing_time', 'N/A')}s")
                print(f"    WebSearch: {metadata.get('websearch_provider', 'N/A')}")
            except:
                print(f"    Metadata: {metadata_json[:100]}...")

    # –≠–∫—Å–ø–æ—Ä—Ç –≤ Word
    if DOCX_AVAILABLE and result_a['status'] == 'completed' and result_b['status'] == 'completed':
        print("\n" + "=" * 80)
        print("üìÑ –≠–ö–°–ü–û–†–¢: –°–æ–∑–¥–∞–Ω–∏–µ Word –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("=" * 80)

        output_dir = Path("grants_output/ab_test_ekaterina")
        output_dir.mkdir(parents=True, exist_ok=True)

        # –≠–∫—Å–ø–æ—Ä—Ç VERSION A
        file_a = output_dir / f"Research_A_{result_a['research_id']}.docx"
        export_to_word(result_a, file_a, "VERSION A")
        print(f"\n‚úÖ VERSION A —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞:")
        print(f"   {file_a}")

        # –≠–∫—Å–ø–æ—Ä—Ç VERSION B
        file_b = output_dir / f"Research_B_{result_b['research_id']}.docx"
        export_to_word(result_b, file_b, "VERSION B")
        print(f"\n‚úÖ VERSION B —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞:")
        print(f"   {file_b}")

        # –°–æ–∑–¥–∞—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        comparison_file = output_dir / f"Comparison_AB_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
        create_comparison_document(result_a, result_b, comparison_file)
        print(f"\n‚úÖ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω:")
        print(f"   {comparison_file}")

        print("\n" + "=" * 80)
        print("üìÅ –§–ê–ô–õ–´ –î–õ–Ø –ß–¢–ï–ù–ò–Ø:")
        print("=" * 80)
        print(f"\n1. VERSION A: {file_a}")
        print(f"2. VERSION B: {file_b}")
        print(f"3. COMPARISON: {comparison_file}")

        return {
            'version_a': str(file_a),
            'version_b': str(file_b),
            'comparison': str(comparison_file),
            'research_ids': [result_a['research_id'], result_b['research_id']]
        }

    return {
        'research_ids': [result_a.get('research_id'), result_b.get('research_id')],
        'statuses': [result_a['status'], result_b['status']]
    }


def export_to_word(research_result: dict, output_path: Path, version_label: str):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ Word –¥–æ–∫—É–º–µ–Ω—Ç"""

    doc = Document()

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title = doc.add_heading(f'–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω—Ç–æ–≤–æ–π –∑–∞—è–≤–∫–∏ - {version_label}', 0)
    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    doc.add_heading('–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è', 1)

    metadata = research_result['research_results']['metadata']

    table = doc.add_table(rows=7, cols=2)
    table.style = 'Light Grid Accent 1'

    rows_data = [
        ('Research ID', research_result['research_id']),
        ('–°—Ç–∞—Ç—É—Å', research_result['status']),
        ('–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤', str(metadata['total_queries'])),
        ('–í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤', str(metadata['sources_count'])),
        ('–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏', f"{metadata['total_processing_time']}s"),
        ('WebSearch –ø—Ä–æ–≤–∞–π–¥–µ—Ä', metadata.get('websearch_provider', 'unknown')),
        ('Fallback –ø—Ä–æ–≤–∞–π–¥–µ—Ä', metadata.get('websearch_fallback', 'none'))
    ]

    for idx, (label, value) in enumerate(rows_data):
        table.rows[idx].cells[0].text = label
        table.rows[idx].cells[1].text = value

    doc.add_paragraph()

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–ª–æ–∫–æ–≤
    for block_key in ['block1_problem', 'block2_geography', 'block3_goals']:
        block_data = research_result['research_results'].get(block_key, {})

        if not block_data:
            continue

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–ª–æ–∫–∞
        block_names = {
            'block1_problem': '–ë–ª–æ–∫ 1: –ü—Ä–æ–±–ª–µ–º–∞ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å',
            'block2_geography': '–ë–ª–æ–∫ 2: –ì–µ–æ–≥—Ä–∞—Ñ–∏—è –∏ —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è',
            'block3_goals': '–ë–ª–æ–∫ 3: –ó–∞–¥–∞—á–∏, –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏ –≥–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å'
        }

        doc.add_heading(block_names[block_key], 1)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–ª–æ–∫–∞
        doc.add_paragraph(f"–ó–∞–ø—Ä–æ—Å–æ–≤: {len(block_data.get('queries_used', []))}")
        doc.add_paragraph(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {block_data.get('total_sources', 0)}")
        doc.add_paragraph(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {block_data.get('processing_time', 0)}s")

        # –†–µ–∑—é–º–µ
        doc.add_heading('–†–µ–∑—é–º–µ', 2)
        doc.add_paragraph(block_data.get('summary', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'))

        # –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã
        key_facts = block_data.get('key_facts', [])
        if key_facts:
            doc.add_heading('–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã', 2)
            for idx, fact in enumerate(key_facts[:10], 1):
                p = doc.add_paragraph(style='List Number')
                p.add_run(f"{fact.get('fact', '')}\n")
                p.add_run(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {fact.get('source', 'unknown')}").font.size = Pt(9)

        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        queries_used = block_data.get('queries_used', [])
        if queries_used:
            doc.add_heading('–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã', 2)
            for idx, query in enumerate(queries_used, 1):
                doc.add_paragraph(f"{idx}. {query}", style='List Number')

        doc.add_page_break()

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
    doc.save(str(output_path))


def create_comparison_document(result_a: dict, result_b: dict, output_path: Path):
    """–°–æ–∑–¥–∞—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è A –∏ B –≤–µ—Ä—Å–∏–π"""

    doc = Document()

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title = doc.add_heading('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è A –∏ B', 0)
    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER

    doc.add_paragraph(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph()

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    doc.add_heading('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö', 1)

    metadata_a = result_a['research_results']['metadata']
    metadata_b = result_b['research_results']['metadata']

    table = doc.add_table(rows=8, cols=3)
    table.style = 'Light Grid Accent 1'

    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    table.rows[0].cells[0].text = '–ü–∞—Ä–∞–º–µ—Ç—Ä'
    table.rows[0].cells[1].text = 'VERSION A'
    table.rows[0].cells[2].text = 'VERSION B'

    # –î–∞–Ω–Ω—ã–µ
    comparison_data = [
        ('Research ID', result_a['research_id'], result_b['research_id']),
        ('–°—Ç–∞—Ç—É—Å', result_a['status'], result_b['status']),
        ('–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤', metadata_a['total_queries'], metadata_b['total_queries']),
        ('–í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤', metadata_a['sources_count'], metadata_b['sources_count']),
        ('–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (s)', metadata_a['total_processing_time'], metadata_b['total_processing_time']),
        ('WebSearch –ø—Ä–æ–≤–∞–π–¥–µ—Ä', metadata_a.get('websearch_provider', 'unknown'), metadata_b.get('websearch_provider', 'unknown')),
        ('Fallback –ø—Ä–æ–≤–∞–π–¥–µ—Ä', metadata_a.get('websearch_fallback', 'none'), metadata_b.get('websearch_fallback', 'none'))
    ]

    for idx, (label, val_a, val_b) in enumerate(comparison_data, 1):
        table.rows[idx].cells[0].text = label
        table.rows[idx].cells[1].text = str(val_a)
        table.rows[idx].cells[2].text = str(val_b)

    doc.add_paragraph()

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤
    doc.add_heading('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤', 1)

    for block_key in ['block1_problem', 'block2_geography', 'block3_goals']:
        block_names = {
            'block1_problem': '–ë–ª–æ–∫ 1: –ü—Ä–æ–±–ª–µ–º–∞',
            'block2_geography': '–ë–ª–æ–∫ 2: –ì–µ–æ–≥—Ä–∞—Ñ–∏—è',
            'block3_goals': '–ë–ª–æ–∫ 3: –ó–∞–¥–∞—á–∏ –∏ —Ü–µ–ª–∏'
        }

        doc.add_heading(block_names[block_key], 2)

        block_a = result_a['research_results'].get(block_key, {})
        block_b = result_b['research_results'].get(block_key, {})

        table = doc.add_table(rows=4, cols=3)
        table.style = 'Light Grid Accent 1'

        table.rows[0].cells[0].text = '–ú–µ—Ç—Ä–∏–∫–∞'
        table.rows[0].cells[1].text = 'VERSION A'
        table.rows[0].cells[2].text = 'VERSION B'

        table.rows[1].cells[0].text = '–ó–∞–ø—Ä–æ—Å–æ–≤'
        table.rows[1].cells[1].text = str(len(block_a.get('queries_used', [])))
        table.rows[1].cells[2].text = str(len(block_b.get('queries_used', [])))

        table.rows[2].cells[0].text = '–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤'
        table.rows[2].cells[1].text = str(block_a.get('total_sources', 0))
        table.rows[2].cells[2].text = str(block_b.get('total_sources', 0))

        table.rows[3].cells[0].text = '–í—Ä–µ–º—è (s)'
        table.rows[3].cells[1].text = str(block_a.get('processing_time', 0))
        table.rows[3].cells[2].text = str(block_b.get('processing_time', 0))

        doc.add_paragraph()

    # –í—ã–≤–æ–¥—ã
    doc.add_page_break()
    doc.add_heading('–í—ã–≤–æ–¥—ã', 1)

    doc.add_paragraph(f"‚úÖ –û–±–µ –≤–µ—Ä—Å–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î")
    doc.add_paragraph(f"‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö research –¥–ª—è –æ–¥–Ω–æ–π anketa_id")
    doc.add_paragraph(f"‚úÖ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞")

    # –†–∞–∑–ª–∏—á–∏—è
    sources_diff = abs(metadata_a['sources_count'] - metadata_b['sources_count'])
    time_diff = abs(metadata_a['total_processing_time'] - metadata_b['total_processing_time'])

    doc.add_paragraph(f"\n–†–∞–∑–ª–∏—á–∏—è:")
    doc.add_paragraph(f"- –ò—Å—Ç–æ—á–Ω–∏–∫–∏: ¬±{sources_diff}")
    doc.add_paragraph(f"- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: ¬±{time_diff}s")

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
    doc.save(str(output_path))


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("üöÄ –ó–ê–ü–£–°–ö A/B –¢–ï–°–¢–ê")
    print("=" * 80)
    print(f"–í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"–ó–∞–¥–∞—á–∞: 2 –ø–æ–ª–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–∞ researcher –¥–ª—è –∞–Ω–∫–µ—Ç—ã –ï–∫–∞—Ç–µ—Ä–∏–Ω—ã (27 queries √ó 2)")
    print(f"–¶–µ–ª—å: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö research versions")
    print("=" * 80)

    try:
        result = asyncio.run(run_ab_test())

        print("\n" + "=" * 80)
        print("‚úÖ A/B –¢–ï–°–¢ –ó–ê–í–ï–†–®–Å–ù –£–°–ü–ï–®–ù–û!")
        print("=" * 80)

        if 'version_a' in result:
            print("\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            print(f"1. {result['version_a']}")
            print(f"2. {result['version_b']}")
            print(f"3. {result['comparison']}")

        print(f"\nüî¨ Research IDs: {', '.join(result['research_ids'])}")

    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
